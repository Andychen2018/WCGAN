{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021年5月11日 改进CGAN为WCGAN网络,用于生成二分类的数据,这里的数据用的是我们整理出的用电数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision \n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import  numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE=32\n",
    "IMG_SIZE=67\n",
    "CHANNELS_IMG=1\n",
    "NUM_CLASSES = 2\n",
    "GEN_EMBEDDING = 100\n",
    "Z_DIM=100\n",
    "NUM_EPOCHS=500\n",
    "FEATURES_DISC=16\n",
    "FEATURES_GEN=16\n",
    "CRITIC_ITERATIONS =5\n",
    "LAMBDA_GP = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDataset(Dataset):\n",
    "    def __init__(self,csv_file):\n",
    "        self.data_df = pd.read_csv(csv_file,header = 0)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def  __len__(self):\n",
    "        return len(self.data_df)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        label = self.data_df.iloc[index,0]\n",
    "\n",
    "        target = torch.zeros((2))\n",
    "        target[label] = 1.0\n",
    "        #image_values = torch.FloatTensor(self.data_df.iloc[index,1:].values)/255.0\n",
    "        image_values = torch.FloatTensor(self.data_df.iloc[index,1:4490].values)/6108.0\n",
    "        image_values = torch.FloatTensor(image_values).view(1,67,67)\n",
    "        UserID = self.data_df.index[index]\n",
    "        \n",
    "        sample = {'label':label,'image_values':image_values,'target':target,'UserID':UserID}\n",
    "        #return sample\n",
    "        return label,image_values,target,UserID,sample\n",
    "    \n",
    "    def plot_image(self,index):\n",
    "        img = self.data_df.iloc[index,1:4490].values.reshape(67,67)\n",
    "        plt.title(\"User\" + str(self.data_df.index[index])+ \"  Label=\"+ str(self.data_df.iloc[index,0]))\n",
    "        #plt.title(\"label = \" + str(self.data_df.iloc[index,0]))\n",
    "        plt.imshow(img,interpolation='none',cmap = 'Blues')\n",
    "        pass\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset = MnistDataset('./FullDataSet1.csv')\n",
    "#mnist_dataset.plot_image(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(4):\n",
    "#     sample = mnist_dataset[i]\n",
    "#     print(i,sample['image'].shape,sample['label'],sample['target'],sample['UserID'])\n",
    "#     ax = plt.subplot(2,2,i+1)\n",
    "#     plt.tight_layout()\n",
    "#     plt.imshow((sample['image'].reshape(67,67)),interpolation='none',cmap = 'Blues')\n",
    "#     plt.title(\"True Label:\" +str(sample['label']) +\"  UsrID:\" +str(sample['UserID']))\n",
    "#     #sample.plot_image(i)\n",
    "#     #plt.imshow(sample[i].image_values.reshape(67,67),cmap = 'gray',interpolation='none')\n",
    "#     #plt.title(\"Ground Truth:{}\".format(sample[i]))\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for label, image_data_tensor, label_tensor in mnist_dataset:\n",
    "#     print(label)\n",
    "#     print(label_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########这里是为了查看Dataset和DataLoader两个类的作用，mnist_dataset是要处理的数据类，batch_size是每批次需要处理的数据个数\n",
    "###########这里DataLoader的重点需要看for 循环后的enumerate，即for index,data in enumerate(loader):这里for循环后只有两个变量，\n",
    "###########index指示该次循环，data表示这一index次抽取出的数据，是个词典，它把所有的数据都拿过来，\n",
    "###########我们使用data[0],data[1],data[2],data[3],data[4]来表示词典中返回的数值，如原来保存的标签，数值等内容\n",
    "loader =DataLoader(mnist_dataset,batch_size=BATCH_SIZE,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,data in enumerate(loader):\n",
    "#     label, value,target,ID= data[0],data[1],data[2],data[3]\n",
    "#     print(\"Index=\",i,\"Label:\",label,\"ImageValue:\",value,\" Target=\",target,\"UserID=\",ID)#####这里因为100个用户分批次训练 ，最终剩下4个用户而导致这里的数据有变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic,labels,real,fake,device=\"cpu\"):\n",
    "    BATCH_SIZE,C,H,W = real.shape\n",
    "    epsilon = torch.rand((BATCH_SIZE,1,1,1)).repeat(1,C,H,W).to(device)\n",
    "    interpolated_images = real*epsilon + fake*(1-epsilon)\n",
    "    \n",
    "    mixed_scores = critic(interpolated_images,labels)\n",
    "    \n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs = interpolated_images,\n",
    "        outputs = mixed_scores,\n",
    "        grad_outputs = torch.ones_like(mixed_scores),\n",
    "        create_graph = True,\n",
    "        retain_graph = True,      \n",
    "    )[0]\n",
    "    \n",
    "    \n",
    "    gradient = gradient.view(gradient.shape[0],-1)\n",
    "    gradient_norm = gradient.norm(2,dim =1)\n",
    "    gradient_penalty = torch.mean((gradient_norm -1)**2)\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,channels_img,features_d,num_classes,img_size):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.disc = nn.Sequential(\n",
    "                              # input N*channels_img*64*64\n",
    "            nn.Conv2d(channels_img+1,features_d,kernel_size=4,stride=2,padding=1), #32*32\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            self._block(features_d,features_d*2,4,2,1),#16*16\n",
    "            self._block(features_d*2,features_d*4,4,2,1),#8*8\n",
    "            self._block(features_d*4,features_d*8,4,2,1),#4*4\n",
    "            nn.Conv2d(features_d*8,1,kernel_size=4,stride=2,padding=0), #1*1       \n",
    "                \n",
    "        )\n",
    "        self.embed = nn.Embedding(num_classes,img_size*img_size)\n",
    "    def _block(self,in_channels,out_channels,kernel_size,stride,padding):\n",
    "        return nn.Sequential(\n",
    "        nn.Conv2d(\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        padding,\n",
    "        bias=False,),\n",
    "        nn.InstanceNorm2d(out_channels,affine=True),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        )\n",
    "    def forward(self,x,labels):\n",
    "        \n",
    "        embedding = self.embed(labels).view(labels.shape[0],1,self.img_size,self.img_size)\n",
    "        x = torch.cat([x,embedding],dim=1)\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,\n",
    "                 channels_noise,\n",
    "                 channels_img,\n",
    "                 features_g,\n",
    "                 num_classes,\n",
    "                 img_size,\n",
    "                 embed_size,\n",
    "                ):\n",
    "        super(Generator,self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.gen = nn.Sequential(\n",
    "                            # input N*z_dim*1*1\n",
    "            self._block(channels_noise+embed_size,features_g*16,4,1,0),#N*f_g*16*4*4\n",
    "            self._block(features_g*16,features_g*8,4,2,1),#8*8\n",
    "            self._block(features_g*8,features_g*4,4,2,1),#16*16\n",
    "            self._block(features_g*4,features_g*2,4,2,1),#32*32\n",
    "            \n",
    "            nn.ConvTranspose2d(features_g*2,channels_img,kernel_size=5,stride=2,padding=0,) ,#1*1       \n",
    "            nn.Tanh(),#[-1,1]\n",
    "        )\n",
    "        self.embed = nn.Embedding(num_classes,embed_size)\n",
    "        \n",
    "    def _block(self,in_channels,out_channels,kernel_size,stride,padding):\n",
    "        return nn.Sequential(\n",
    "        nn.ConvTranspose2d(\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        padding,\n",
    "        bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(),\n",
    "        )\n",
    "    def forward(self,x,labels):\n",
    "        #latent vector z:N*noise_dim*1*1\n",
    "        embedding = self.embed(labels).unsqueeze(2).unsqueeze(3)\n",
    "        x = torch.cat([x,embedding],dim =1)\n",
    "        return self.gen(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m,(nn.Conv2d,nn.ConvTranspose2d,nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data,0.0,0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = MnistDataset('./FullDataSet1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('dataset/',\n",
    "#                             train=False,download=True,transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "#                             torchvision.transforms.Normalize((0.1307,),(0.3081,))])),batch_size=64,shuffle=True)\n",
    "# examples= enumerate(test_loader)\n",
    "# batch_idx,(real,labels) = next(examples)\n",
    "# fig = plt.figure()\n",
    "# for i in range(6):\n",
    "#     plt.subplot(2,3,i+1)\n",
    "#     plt.tight_layout()\n",
    "#     plt.imshow(example_data[i][0],cmap = 'gray',interpolation='none')\n",
    "#     plt.title(\"Ground Truth:{}\".format(example_targets[i]))\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################这里是为了测试生成随机标签的Tensor数据，这里需要注意random_idx = random.randint(0,1)，\n",
    "# #################它的生成范围应该是0-1之间。从这里能够看出，标签为1时，对应的OneHot编码为[0,1],标签为0时，对应的OneHot编码为[1,0],\n",
    "# label_tensor = torch.zeros((2))\n",
    "# print(label_tensor)\n",
    "# random_idx = random.randint(0,1)\n",
    "# print(random_idx)\n",
    "# label_tensor[random_idx]=1.0\n",
    "# print(label_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(Z_DIM,CHANNELS_IMG,FEATURES_GEN,NUM_CLASSES,IMG_SIZE,GEN_EMBEDDING).to(device)\n",
    "critic = Discriminator(CHANNELS_IMG,FEATURES_GEN,NUM_CLASSES,IMG_SIZE).to(device)\n",
    "initialize_weights(gen)\n",
    "initialize_weights(critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_gen = optim.Adam(gen.parameters(),lr=LEARNING_RATE,betas=(0.0,0.999))\n",
    "opt_critic = optim.Adam(critic.parameters(),lr=LEARNING_RATE,betas=(0.0,0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(32,Z_DIM,1,1).to(device)\n",
    "writer_real = SummaryWriter(f\"/home/ncepu/sunmingyu/chenzhiqian/Power-prediction/WGANs/CGAN03/real\")\n",
    "writer_fake = SummaryWriter(f\"/home/ncepu/sunmingyu/chenzhiqian/Power-prediction/WGANs/CGAN03/fake\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_idx,data in enumerate(loader):\n",
    "#     real = data[1]\n",
    "#     cur_batch_size = real.shape[0]\n",
    "#     labels = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('real=',real.shape)\n",
    "# print('cur_batch_size=',cur_batch_size)\n",
    "# print('labels=',labels,labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise = torch.randn((cur_batch_size,Z_DIM,1,1))\n",
    "# print(noise.shape)\n",
    "# fake = gen(noise,labels)\n",
    "# print(fake.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (gen): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose2d(200, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (4): ConvTranspose2d(32, 1, kernel_size=(5, 5), stride=(2, 2))\n",
       "    (5): Tanh()\n",
       "  )\n",
       "  (embed): Embedding(2, 100)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (disc): Sequential(\n",
       "    (0): Conv2d(2, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (5): Conv2d(128, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "  )\n",
       "  (embed): Embedding(2, 4489)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/500] Batch 0/133                Loss D: -0.0424,loss G: 0.5304\n",
      "Epoch [0/500] Batch 100/133                Loss D: 0.1810,loss G: -0.1971\n",
      "Epoch [1/500] Batch 0/133                Loss D: 0.1712,loss G: -0.0703\n",
      "Epoch [1/500] Batch 100/133                Loss D: 0.2044,loss G: -0.0161\n",
      "Epoch [2/500] Batch 0/133                Loss D: 0.3919,loss G: -1.0810\n",
      "Epoch [2/500] Batch 100/133                Loss D: 0.1789,loss G: -2.6590\n",
      "Epoch [3/500] Batch 0/133                Loss D: 0.2101,loss G: -1.9397\n",
      "Epoch [3/500] Batch 100/133                Loss D: 0.1410,loss G: -2.3893\n",
      "Epoch [4/500] Batch 0/133                Loss D: 0.1639,loss G: -1.8506\n",
      "Epoch [4/500] Batch 100/133                Loss D: 0.1189,loss G: -2.7353\n",
      "Epoch [5/500] Batch 0/133                Loss D: 0.0943,loss G: -2.7846\n",
      "Epoch [5/500] Batch 100/133                Loss D: 0.1099,loss G: -2.6172\n",
      "Epoch [6/500] Batch 0/133                Loss D: 0.0435,loss G: -2.5740\n",
      "Epoch [6/500] Batch 100/133                Loss D: 0.0562,loss G: -2.4956\n",
      "Epoch [7/500] Batch 0/133                Loss D: 0.0437,loss G: -2.2506\n",
      "Epoch [7/500] Batch 100/133                Loss D: 0.0341,loss G: -2.8793\n",
      "Epoch [8/500] Batch 0/133                Loss D: 0.0367,loss G: -2.8040\n",
      "Epoch [8/500] Batch 100/133                Loss D: 0.0052,loss G: -2.3878\n",
      "Epoch [9/500] Batch 0/133                Loss D: 0.0184,loss G: -1.6282\n",
      "Epoch [9/500] Batch 100/133                Loss D: 0.0873,loss G: -1.8637\n",
      "Epoch [10/500] Batch 0/133                Loss D: 0.0019,loss G: -1.6070\n",
      "Epoch [10/500] Batch 100/133                Loss D: 0.0251,loss G: -1.2776\n",
      "Epoch [11/500] Batch 0/133                Loss D: 0.0005,loss G: -1.4083\n",
      "Epoch [11/500] Batch 100/133                Loss D: 0.0009,loss G: -1.7906\n",
      "Epoch [12/500] Batch 0/133                Loss D: -0.0018,loss G: -1.8484\n",
      "Epoch [12/500] Batch 100/133                Loss D: 0.0066,loss G: -1.3655\n",
      "Epoch [13/500] Batch 0/133                Loss D: -0.0087,loss G: -1.2662\n",
      "Epoch [13/500] Batch 100/133                Loss D: -0.0023,loss G: -1.1559\n",
      "Epoch [14/500] Batch 0/133                Loss D: -0.0089,loss G: -1.5030\n",
      "Epoch [14/500] Batch 100/133                Loss D: -0.0022,loss G: -1.0901\n",
      "Epoch [15/500] Batch 0/133                Loss D: 0.0039,loss G: -1.1613\n",
      "Epoch [15/500] Batch 100/133                Loss D: 0.0197,loss G: -1.4699\n",
      "Epoch [16/500] Batch 0/133                Loss D: -0.0021,loss G: -1.2383\n",
      "Epoch [16/500] Batch 100/133                Loss D: -0.0043,loss G: -1.0522\n",
      "Epoch [17/500] Batch 0/133                Loss D: 0.0329,loss G: -1.4056\n",
      "Epoch [17/500] Batch 100/133                Loss D: 0.0087,loss G: -1.1343\n",
      "Epoch [18/500] Batch 0/133                Loss D: 0.0119,loss G: -1.0054\n",
      "Epoch [18/500] Batch 100/133                Loss D: 0.0144,loss G: -0.9857\n",
      "Epoch [19/500] Batch 0/133                Loss D: 0.0130,loss G: -0.9802\n",
      "Epoch [19/500] Batch 100/133                Loss D: 0.0028,loss G: -0.4226\n",
      "Epoch [20/500] Batch 0/133                Loss D: -0.0038,loss G: -0.5047\n",
      "Epoch [20/500] Batch 100/133                Loss D: -0.0078,loss G: -0.5940\n",
      "Epoch [21/500] Batch 0/133                Loss D: 0.0094,loss G: -0.4840\n",
      "Epoch [21/500] Batch 100/133                Loss D: -0.0119,loss G: -0.3241\n",
      "Epoch [22/500] Batch 0/133                Loss D: -0.0144,loss G: -0.1758\n",
      "Epoch [22/500] Batch 100/133                Loss D: -0.0139,loss G: -0.2462\n",
      "Epoch [23/500] Batch 0/133                Loss D: 0.0081,loss G: -0.3329\n",
      "Epoch [23/500] Batch 100/133                Loss D: 0.0052,loss G: -0.3758\n",
      "Epoch [24/500] Batch 0/133                Loss D: -0.0042,loss G: -0.2535\n",
      "Epoch [24/500] Batch 100/133                Loss D: 0.0156,loss G: 0.4097\n",
      "Epoch [25/500] Batch 0/133                Loss D: 0.0003,loss G: 0.1660\n",
      "Epoch [25/500] Batch 100/133                Loss D: 0.0310,loss G: -0.0776\n",
      "Epoch [26/500] Batch 0/133                Loss D: 0.0465,loss G: 0.2274\n",
      "Epoch [26/500] Batch 100/133                Loss D: 0.0042,loss G: 0.3063\n",
      "Epoch [27/500] Batch 0/133                Loss D: 0.0040,loss G: 0.4016\n",
      "Epoch [27/500] Batch 100/133                Loss D: 0.0051,loss G: 0.3665\n",
      "Epoch [28/500] Batch 0/133                Loss D: 0.0113,loss G: -0.0726\n",
      "Epoch [28/500] Batch 100/133                Loss D: 0.1723,loss G: -0.7009\n",
      "Epoch [29/500] Batch 0/133                Loss D: 0.0680,loss G: -0.7939\n",
      "Epoch [29/500] Batch 100/133                Loss D: 0.1242,loss G: -1.4796\n",
      "Epoch [30/500] Batch 0/133                Loss D: 0.1421,loss G: -2.5668\n",
      "Epoch [30/500] Batch 100/133                Loss D: 0.1573,loss G: -4.5616\n",
      "Epoch [31/500] Batch 0/133                Loss D: 0.1700,loss G: -2.8634\n",
      "Epoch [31/500] Batch 100/133                Loss D: 0.1147,loss G: -1.8412\n",
      "Epoch [32/500] Batch 0/133                Loss D: 0.2046,loss G: -2.2766\n",
      "Epoch [32/500] Batch 100/133                Loss D: 0.1079,loss G: -3.9971\n",
      "Epoch [33/500] Batch 0/133                Loss D: 0.0616,loss G: -3.5156\n",
      "Epoch [33/500] Batch 100/133                Loss D: 0.0355,loss G: -2.0632\n",
      "Epoch [34/500] Batch 0/133                Loss D: 0.0004,loss G: -2.7114\n",
      "Epoch [34/500] Batch 100/133                Loss D: 0.0063,loss G: -3.2205\n",
      "Epoch [35/500] Batch 0/133                Loss D: 0.0188,loss G: -2.7455\n",
      "Epoch [35/500] Batch 100/133                Loss D: 0.0204,loss G: -2.2542\n",
      "Epoch [36/500] Batch 0/133                Loss D: -0.0107,loss G: -1.9612\n",
      "Epoch [36/500] Batch 100/133                Loss D: -0.0052,loss G: -1.9265\n",
      "Epoch [37/500] Batch 0/133                Loss D: -0.0024,loss G: -1.9396\n",
      "Epoch [37/500] Batch 100/133                Loss D: -0.0033,loss G: -2.0805\n",
      "Epoch [38/500] Batch 0/133                Loss D: -0.0050,loss G: -1.9240\n",
      "Epoch [38/500] Batch 100/133                Loss D: 0.0720,loss G: -1.3243\n",
      "Epoch [39/500] Batch 0/133                Loss D: -0.0031,loss G: -1.6334\n",
      "Epoch [39/500] Batch 100/133                Loss D: -0.0010,loss G: -1.5901\n",
      "Epoch [40/500] Batch 0/133                Loss D: -0.0052,loss G: -1.6037\n",
      "Epoch [40/500] Batch 100/133                Loss D: -0.0079,loss G: -1.1374\n",
      "Epoch [41/500] Batch 0/133                Loss D: 0.0043,loss G: -1.0592\n",
      "Epoch [41/500] Batch 100/133                Loss D: 0.0014,loss G: -0.9599\n",
      "Epoch [42/500] Batch 0/133                Loss D: -0.0072,loss G: -0.4745\n",
      "Epoch [42/500] Batch 100/133                Loss D: -0.0067,loss G: -0.6438\n",
      "Epoch [43/500] Batch 0/133                Loss D: -0.0007,loss G: -0.5435\n",
      "Epoch [43/500] Batch 100/133                Loss D: 0.0218,loss G: -0.7533\n",
      "Epoch [44/500] Batch 0/133                Loss D: -0.0016,loss G: -0.8289\n",
      "Epoch [44/500] Batch 100/133                Loss D: -0.0065,loss G: -0.8177\n",
      "Epoch [45/500] Batch 0/133                Loss D: -0.0067,loss G: -0.8585\n",
      "Epoch [45/500] Batch 100/133                Loss D: -0.0015,loss G: -0.5175\n",
      "Epoch [46/500] Batch 0/133                Loss D: -0.0052,loss G: -0.1076\n",
      "Epoch [46/500] Batch 100/133                Loss D: 0.0009,loss G: 0.2750\n",
      "Epoch [47/500] Batch 0/133                Loss D: -0.0001,loss G: -0.2377\n",
      "Epoch [47/500] Batch 100/133                Loss D: -0.0016,loss G: -0.5084\n",
      "Epoch [48/500] Batch 0/133                Loss D: -0.0027,loss G: -0.5108\n",
      "Epoch [48/500] Batch 100/133                Loss D: -0.0016,loss G: -0.4612\n",
      "Epoch [49/500] Batch 0/133                Loss D: 0.0108,loss G: -0.1810\n",
      "Epoch [49/500] Batch 100/133                Loss D: 0.1501,loss G: 0.4053\n",
      "Epoch [50/500] Batch 0/133                Loss D: 0.0041,loss G: 0.3498\n",
      "Epoch [50/500] Batch 100/133                Loss D: -0.0041,loss G: 0.4689\n",
      "Epoch [51/500] Batch 0/133                Loss D: -0.0114,loss G: 0.4658\n",
      "Epoch [51/500] Batch 100/133                Loss D: -0.0028,loss G: 0.3511\n",
      "Epoch [52/500] Batch 0/133                Loss D: -0.0034,loss G: 0.2187\n",
      "Epoch [52/500] Batch 100/133                Loss D: -0.0093,loss G: -0.0848\n",
      "Epoch [53/500] Batch 0/133                Loss D: -0.0106,loss G: 0.0508\n",
      "Epoch [53/500] Batch 100/133                Loss D: 0.0034,loss G: 0.4678\n",
      "Epoch [54/500] Batch 0/133                Loss D: 0.0011,loss G: 0.6396\n",
      "Epoch [54/500] Batch 100/133                Loss D: 0.0004,loss G: 0.4832\n",
      "Epoch [55/500] Batch 0/133                Loss D: -0.0016,loss G: 0.3277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/500] Batch 100/133                Loss D: 0.0004,loss G: 0.0833\n",
      "Epoch [56/500] Batch 0/133                Loss D: 0.0049,loss G: -0.1892\n",
      "Epoch [56/500] Batch 100/133                Loss D: 0.0107,loss G: -0.9765\n",
      "Epoch [57/500] Batch 0/133                Loss D: 0.0207,loss G: -0.8394\n",
      "Epoch [57/500] Batch 100/133                Loss D: 0.0040,loss G: -1.3853\n",
      "Epoch [58/500] Batch 0/133                Loss D: 0.0079,loss G: -1.1532\n",
      "Epoch [58/500] Batch 100/133                Loss D: 0.0244,loss G: -1.5079\n",
      "Epoch [59/500] Batch 0/133                Loss D: 0.0258,loss G: -2.0321\n",
      "Epoch [59/500] Batch 100/133                Loss D: 0.0416,loss G: -1.7675\n",
      "Epoch [60/500] Batch 0/133                Loss D: 0.0277,loss G: -2.1996\n",
      "Epoch [60/500] Batch 100/133                Loss D: 0.0207,loss G: -1.1129\n",
      "Epoch [61/500] Batch 0/133                Loss D: 0.0090,loss G: -0.1629\n",
      "Epoch [61/500] Batch 100/133                Loss D: 0.0075,loss G: -0.7453\n",
      "Epoch [62/500] Batch 0/133                Loss D: 0.0186,loss G: -0.3329\n",
      "Epoch [62/500] Batch 100/133                Loss D: 0.0022,loss G: -0.0577\n",
      "Epoch [63/500] Batch 0/133                Loss D: 0.0004,loss G: -0.3339\n",
      "Epoch [63/500] Batch 100/133                Loss D: -0.0027,loss G: -0.3428\n",
      "Epoch [64/500] Batch 0/133                Loss D: -0.0037,loss G: -0.5558\n",
      "Epoch [64/500] Batch 100/133                Loss D: -0.0074,loss G: -0.9916\n",
      "Epoch [65/500] Batch 0/133                Loss D: 0.0051,loss G: -0.6731\n",
      "Epoch [65/500] Batch 100/133                Loss D: -0.0034,loss G: -0.6191\n",
      "Epoch [66/500] Batch 0/133                Loss D: 0.0170,loss G: -0.6052\n",
      "Epoch [66/500] Batch 100/133                Loss D: -0.0077,loss G: -0.8702\n",
      "Epoch [67/500] Batch 0/133                Loss D: -0.0062,loss G: -0.9093\n",
      "Epoch [67/500] Batch 100/133                Loss D: 0.0071,loss G: -0.3945\n",
      "Epoch [68/500] Batch 0/133                Loss D: 0.0013,loss G: -0.0933\n",
      "Epoch [68/500] Batch 100/133                Loss D: -0.0070,loss G: -0.7592\n",
      "Epoch [69/500] Batch 0/133                Loss D: -0.0003,loss G: -0.8588\n",
      "Epoch [69/500] Batch 100/133                Loss D: -0.0059,loss G: -1.0774\n",
      "Epoch [70/500] Batch 0/133                Loss D: 0.0446,loss G: -1.0150\n",
      "Epoch [70/500] Batch 100/133                Loss D: 0.0045,loss G: -0.5419\n",
      "Epoch [71/500] Batch 0/133                Loss D: 0.0169,loss G: -0.0976\n",
      "Epoch [71/500] Batch 100/133                Loss D: 0.0103,loss G: 0.2797\n",
      "Epoch [72/500] Batch 0/133                Loss D: 0.0087,loss G: -0.3218\n",
      "Epoch [72/500] Batch 100/133                Loss D: 0.0093,loss G: -1.2290\n",
      "Epoch [73/500] Batch 0/133                Loss D: 0.0141,loss G: -0.4364\n",
      "Epoch [73/500] Batch 100/133                Loss D: 0.0033,loss G: 0.1725\n",
      "Epoch [74/500] Batch 0/133                Loss D: 0.0193,loss G: -0.0434\n",
      "Epoch [74/500] Batch 100/133                Loss D: 0.0003,loss G: 0.2648\n",
      "Epoch [75/500] Batch 0/133                Loss D: -0.0049,loss G: 0.2616\n",
      "Epoch [75/500] Batch 100/133                Loss D: 0.0330,loss G: 0.1433\n",
      "Epoch [76/500] Batch 0/133                Loss D: 0.0025,loss G: 0.3605\n",
      "Epoch [76/500] Batch 100/133                Loss D: 0.0214,loss G: 0.2759\n",
      "Epoch [77/500] Batch 0/133                Loss D: 0.0253,loss G: 0.1418\n",
      "Epoch [77/500] Batch 100/133                Loss D: 0.0466,loss G: 0.0169\n",
      "Epoch [78/500] Batch 0/133                Loss D: 0.0060,loss G: 0.2628\n",
      "Epoch [78/500] Batch 100/133                Loss D: -0.0059,loss G: 0.0502\n",
      "Epoch [79/500] Batch 0/133                Loss D: -0.0076,loss G: 0.1398\n",
      "Epoch [79/500] Batch 100/133                Loss D: -0.0097,loss G: -0.2790\n",
      "Epoch [80/500] Batch 0/133                Loss D: -0.0009,loss G: -0.2724\n",
      "Epoch [80/500] Batch 100/133                Loss D: -0.0107,loss G: -0.1285\n",
      "Epoch [81/500] Batch 0/133                Loss D: -0.0167,loss G: -0.1592\n",
      "Epoch [81/500] Batch 100/133                Loss D: -0.0120,loss G: -0.2233\n",
      "Epoch [82/500] Batch 0/133                Loss D: -0.0148,loss G: -0.2604\n",
      "Epoch [82/500] Batch 100/133                Loss D: -0.0078,loss G: -0.1173\n",
      "Epoch [83/500] Batch 0/133                Loss D: -0.0085,loss G: -0.3036\n",
      "Epoch [83/500] Batch 100/133                Loss D: -0.0026,loss G: -0.5163\n",
      "Epoch [84/500] Batch 0/133                Loss D: -0.0195,loss G: -0.2778\n",
      "Epoch [84/500] Batch 100/133                Loss D: 0.0609,loss G: -0.0447\n",
      "Epoch [85/500] Batch 0/133                Loss D: 0.0440,loss G: 3.7320\n",
      "Epoch [85/500] Batch 100/133                Loss D: 0.0694,loss G: 2.3883\n",
      "Epoch [86/500] Batch 0/133                Loss D: 0.0702,loss G: 0.8349\n",
      "Epoch [86/500] Batch 100/133                Loss D: 0.0744,loss G: 1.0301\n",
      "Epoch [87/500] Batch 0/133                Loss D: 0.0479,loss G: 0.7678\n",
      "Epoch [87/500] Batch 100/133                Loss D: 0.0386,loss G: 0.0769\n",
      "Epoch [88/500] Batch 0/133                Loss D: 0.0290,loss G: 0.2471\n",
      "Epoch [88/500] Batch 100/133                Loss D: 0.0069,loss G: -0.5611\n",
      "Epoch [89/500] Batch 0/133                Loss D: 0.0158,loss G: -0.5029\n",
      "Epoch [89/500] Batch 100/133                Loss D: 0.0074,loss G: -0.3045\n",
      "Epoch [90/500] Batch 0/133                Loss D: -0.0044,loss G: -0.4140\n",
      "Epoch [90/500] Batch 100/133                Loss D: -0.0065,loss G: -0.3129\n",
      "Epoch [91/500] Batch 0/133                Loss D: -0.0013,loss G: -0.6627\n",
      "Epoch [91/500] Batch 100/133                Loss D: -0.0010,loss G: -0.6278\n",
      "Epoch [92/500] Batch 0/133                Loss D: -0.0024,loss G: -0.6868\n",
      "Epoch [92/500] Batch 100/133                Loss D: -0.0079,loss G: -0.3129\n",
      "Epoch [93/500] Batch 0/133                Loss D: 0.0129,loss G: -0.7797\n",
      "Epoch [93/500] Batch 100/133                Loss D: -0.0025,loss G: -0.0131\n",
      "Epoch [94/500] Batch 0/133                Loss D: 0.0569,loss G: -0.2251\n",
      "Epoch [94/500] Batch 100/133                Loss D: -0.0002,loss G: 0.4102\n",
      "Epoch [95/500] Batch 0/133                Loss D: 0.0008,loss G: 0.6958\n",
      "Epoch [95/500] Batch 100/133                Loss D: -0.0025,loss G: 1.4672\n",
      "Epoch [96/500] Batch 0/133                Loss D: -0.0074,loss G: 0.6538\n",
      "Epoch [96/500] Batch 100/133                Loss D: 0.0005,loss G: 1.5918\n",
      "Epoch [97/500] Batch 0/133                Loss D: -0.0043,loss G: 1.1389\n",
      "Epoch [97/500] Batch 100/133                Loss D: -0.0054,loss G: 0.3967\n",
      "Epoch [98/500] Batch 0/133                Loss D: -0.0042,loss G: 0.1608\n",
      "Epoch [98/500] Batch 100/133                Loss D: -0.0061,loss G: 0.2575\n",
      "Epoch [99/500] Batch 0/133                Loss D: 0.0008,loss G: -0.2233\n",
      "Epoch [99/500] Batch 100/133                Loss D: -0.0010,loss G: -0.1368\n",
      "Epoch [100/500] Batch 0/133                Loss D: -0.0017,loss G: -0.3581\n",
      "Epoch [100/500] Batch 100/133                Loss D: -0.0069,loss G: -0.0165\n",
      "Epoch [101/500] Batch 0/133                Loss D: 0.0091,loss G: -0.1957\n",
      "Epoch [101/500] Batch 100/133                Loss D: -0.0046,loss G: -0.2648\n",
      "Epoch [102/500] Batch 0/133                Loss D: -0.0026,loss G: -0.2283\n",
      "Epoch [102/500] Batch 100/133                Loss D: -0.0075,loss G: -0.3900\n",
      "Epoch [103/500] Batch 0/133                Loss D: -0.0016,loss G: 1.2257\n",
      "Epoch [103/500] Batch 100/133                Loss D: -0.0050,loss G: 0.6991\n",
      "Epoch [104/500] Batch 0/133                Loss D: -0.0036,loss G: 0.2903\n",
      "Epoch [104/500] Batch 100/133                Loss D: -0.0016,loss G: -0.3651\n",
      "Epoch [105/500] Batch 0/133                Loss D: 0.0016,loss G: 0.1871\n",
      "Epoch [105/500] Batch 100/133                Loss D: -0.0080,loss G: -0.2729\n",
      "Epoch [106/500] Batch 0/133                Loss D: 0.0350,loss G: -0.2124\n",
      "Epoch [106/500] Batch 100/133                Loss D: 0.0108,loss G: -0.1924\n",
      "Epoch [107/500] Batch 0/133                Loss D: 0.0775,loss G: 0.0230\n",
      "Epoch [107/500] Batch 100/133                Loss D: -0.0039,loss G: -0.2776\n",
      "Epoch [108/500] Batch 0/133                Loss D: -0.0065,loss G: -0.3669\n",
      "Epoch [108/500] Batch 100/133                Loss D: -0.0093,loss G: -0.5822\n",
      "Epoch [109/500] Batch 0/133                Loss D: -0.0014,loss G: -0.5274\n",
      "Epoch [109/500] Batch 100/133                Loss D: 0.0331,loss G: -0.9029\n",
      "Epoch [110/500] Batch 0/133                Loss D: 0.0171,loss G: -1.2029\n",
      "Epoch [110/500] Batch 100/133                Loss D: -0.0008,loss G: -1.0390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [111/500] Batch 0/133                Loss D: -0.0050,loss G: -0.8438\n",
      "Epoch [111/500] Batch 100/133                Loss D: -0.0077,loss G: -0.7112\n",
      "Epoch [112/500] Batch 0/133                Loss D: -0.0149,loss G: -0.8969\n",
      "Epoch [112/500] Batch 100/133                Loss D: 0.0333,loss G: -0.4810\n",
      "Epoch [113/500] Batch 0/133                Loss D: -0.0418,loss G: -1.1372\n",
      "Epoch [113/500] Batch 100/133                Loss D: 0.0244,loss G: -1.6606\n",
      "Epoch [114/500] Batch 0/133                Loss D: 0.0002,loss G: -3.1181\n",
      "Epoch [114/500] Batch 100/133                Loss D: 0.0004,loss G: -3.0210\n",
      "Epoch [115/500] Batch 0/133                Loss D: -0.0085,loss G: -2.9549\n",
      "Epoch [115/500] Batch 100/133                Loss D: -0.0129,loss G: -3.4535\n",
      "Epoch [116/500] Batch 0/133                Loss D: -0.0021,loss G: -4.1700\n",
      "Epoch [116/500] Batch 100/133                Loss D: -0.0212,loss G: -2.7886\n",
      "Epoch [117/500] Batch 0/133                Loss D: -0.0214,loss G: -3.1668\n",
      "Epoch [117/500] Batch 100/133                Loss D: 0.0608,loss G: -4.4265\n",
      "Epoch [118/500] Batch 0/133                Loss D: 0.0112,loss G: -3.5624\n",
      "Epoch [118/500] Batch 100/133                Loss D: -0.0024,loss G: -3.3919\n",
      "Epoch [119/500] Batch 0/133                Loss D: -0.0068,loss G: -3.1479\n",
      "Epoch [119/500] Batch 100/133                Loss D: -0.0081,loss G: -2.8783\n",
      "Epoch [120/500] Batch 0/133                Loss D: -0.0077,loss G: -2.7085\n",
      "Epoch [120/500] Batch 100/133                Loss D: 0.1136,loss G: -2.4659\n",
      "Epoch [121/500] Batch 0/133                Loss D: -0.0028,loss G: -1.8939\n",
      "Epoch [121/500] Batch 100/133                Loss D: -0.0057,loss G: -1.6661\n",
      "Epoch [122/500] Batch 0/133                Loss D: -0.0077,loss G: -1.6144\n",
      "Epoch [122/500] Batch 100/133                Loss D: -0.0095,loss G: -1.8046\n",
      "Epoch [123/500] Batch 0/133                Loss D: -0.0015,loss G: -1.7510\n",
      "Epoch [123/500] Batch 100/133                Loss D: -0.0060,loss G: -2.3683\n",
      "Epoch [124/500] Batch 0/133                Loss D: -0.0032,loss G: -2.5733\n",
      "Epoch [124/500] Batch 100/133                Loss D: -0.0056,loss G: -2.3419\n",
      "Epoch [125/500] Batch 0/133                Loss D: -0.0055,loss G: -2.2535\n",
      "Epoch [125/500] Batch 100/133                Loss D: -0.0071,loss G: -0.9083\n",
      "Epoch [126/500] Batch 0/133                Loss D: -0.0080,loss G: -1.2521\n",
      "Epoch [126/500] Batch 100/133                Loss D: -0.0058,loss G: -0.6258\n",
      "Epoch [127/500] Batch 0/133                Loss D: -0.0064,loss G: -0.7814\n",
      "Epoch [127/500] Batch 100/133                Loss D: -0.0075,loss G: 0.1094\n",
      "Epoch [128/500] Batch 0/133                Loss D: -0.0069,loss G: -0.0700\n",
      "Epoch [128/500] Batch 100/133                Loss D: -0.0096,loss G: -0.5455\n",
      "Epoch [129/500] Batch 0/133                Loss D: -0.0030,loss G: 0.1715\n",
      "Epoch [129/500] Batch 100/133                Loss D: -0.0098,loss G: -0.7764\n",
      "Epoch [130/500] Batch 0/133                Loss D: -0.0117,loss G: -0.2684\n",
      "Epoch [130/500] Batch 100/133                Loss D: -0.0132,loss G: -0.3560\n",
      "Epoch [131/500] Batch 0/133                Loss D: -0.0141,loss G: -0.7324\n",
      "Epoch [131/500] Batch 100/133                Loss D: -0.0149,loss G: -0.9665\n",
      "Epoch [132/500] Batch 0/133                Loss D: -0.0109,loss G: -1.4580\n",
      "Epoch [132/500] Batch 100/133                Loss D: 0.0677,loss G: -0.5952\n",
      "Epoch [133/500] Batch 0/133                Loss D: 0.0366,loss G: -1.4714\n",
      "Epoch [133/500] Batch 100/133                Loss D: 0.0213,loss G: -1.9835\n",
      "Epoch [134/500] Batch 0/133                Loss D: 0.0152,loss G: -1.4638\n",
      "Epoch [134/500] Batch 100/133                Loss D: -0.0005,loss G: -0.3137\n",
      "Epoch [135/500] Batch 0/133                Loss D: -0.0014,loss G: -0.3171\n",
      "Epoch [135/500] Batch 100/133                Loss D: -0.0022,loss G: -0.5945\n",
      "Epoch [136/500] Batch 0/133                Loss D: -0.0017,loss G: -0.3398\n",
      "Epoch [136/500] Batch 100/133                Loss D: -0.0040,loss G: -0.3344\n",
      "Epoch [137/500] Batch 0/133                Loss D: 0.0091,loss G: -0.4187\n",
      "Epoch [137/500] Batch 100/133                Loss D: 0.0062,loss G: -0.8087\n",
      "Epoch [138/500] Batch 0/133                Loss D: -0.0052,loss G: -0.0951\n",
      "Epoch [138/500] Batch 100/133                Loss D: -0.0076,loss G: 0.3419\n",
      "Epoch [139/500] Batch 0/133                Loss D: -0.0067,loss G: 0.1579\n",
      "Epoch [139/500] Batch 100/133                Loss D: -0.0080,loss G: 1.1296\n",
      "Epoch [140/500] Batch 0/133                Loss D: -0.0051,loss G: 1.0023\n",
      "Epoch [140/500] Batch 100/133                Loss D: -0.0109,loss G: 0.3982\n",
      "Epoch [141/500] Batch 0/133                Loss D: -0.0136,loss G: 0.3673\n",
      "Epoch [141/500] Batch 100/133                Loss D: -0.0025,loss G: -0.2961\n",
      "Epoch [142/500] Batch 0/133                Loss D: -0.0030,loss G: 0.0220\n",
      "Epoch [142/500] Batch 100/133                Loss D: -0.0035,loss G: 0.1109\n",
      "Epoch [143/500] Batch 0/133                Loss D: -0.0039,loss G: -0.3432\n",
      "Epoch [143/500] Batch 100/133                Loss D: -0.0007,loss G: -0.2371\n",
      "Epoch [144/500] Batch 0/133                Loss D: -0.0045,loss G: -0.4769\n",
      "Epoch [144/500] Batch 100/133                Loss D: -0.0036,loss G: -0.8189\n",
      "Epoch [145/500] Batch 0/133                Loss D: -0.0048,loss G: -0.7981\n",
      "Epoch [145/500] Batch 100/133                Loss D: -0.0008,loss G: -0.9092\n",
      "Epoch [146/500] Batch 0/133                Loss D: -0.0038,loss G: -0.7703\n",
      "Epoch [146/500] Batch 100/133                Loss D: -0.0059,loss G: -0.4423\n",
      "Epoch [147/500] Batch 0/133                Loss D: -0.0021,loss G: -0.1918\n",
      "Epoch [147/500] Batch 100/133                Loss D: 0.0102,loss G: -0.8467\n",
      "Epoch [148/500] Batch 0/133                Loss D: -0.0063,loss G: -0.8636\n",
      "Epoch [148/500] Batch 100/133                Loss D: -0.0066,loss G: -0.9764\n",
      "Epoch [149/500] Batch 0/133                Loss D: -0.0060,loss G: -1.0603\n",
      "Epoch [149/500] Batch 100/133                Loss D: -0.0079,loss G: -1.0036\n",
      "Epoch [150/500] Batch 0/133                Loss D: -0.0107,loss G: -0.8667\n",
      "Epoch [150/500] Batch 100/133                Loss D: -0.0066,loss G: -0.7491\n",
      "Epoch [151/500] Batch 0/133                Loss D: -0.0043,loss G: -0.4510\n",
      "Epoch [151/500] Batch 100/133                Loss D: 0.0013,loss G: -0.6545\n",
      "Epoch [152/500] Batch 0/133                Loss D: -0.0054,loss G: -0.3450\n",
      "Epoch [152/500] Batch 100/133                Loss D: -0.0023,loss G: -1.0365\n",
      "Epoch [153/500] Batch 0/133                Loss D: -0.0067,loss G: -1.1869\n",
      "Epoch [153/500] Batch 100/133                Loss D: -0.0068,loss G: -0.9453\n",
      "Epoch [154/500] Batch 0/133                Loss D: 0.0024,loss G: -1.0571\n",
      "Epoch [154/500] Batch 100/133                Loss D: 0.0052,loss G: -0.3130\n",
      "Epoch [155/500] Batch 0/133                Loss D: 0.0067,loss G: -0.8498\n",
      "Epoch [155/500] Batch 100/133                Loss D: -0.0001,loss G: -1.7651\n",
      "Epoch [156/500] Batch 0/133                Loss D: -0.0031,loss G: -1.3196\n",
      "Epoch [156/500] Batch 100/133                Loss D: -0.0034,loss G: -0.9070\n",
      "Epoch [157/500] Batch 0/133                Loss D: 0.0008,loss G: -1.0020\n",
      "Epoch [157/500] Batch 100/133                Loss D: -0.0020,loss G: -0.5671\n",
      "Epoch [158/500] Batch 0/133                Loss D: 0.0069,loss G: -0.6493\n",
      "Epoch [158/500] Batch 100/133                Loss D: -0.0077,loss G: -0.4570\n",
      "Epoch [159/500] Batch 0/133                Loss D: -0.0034,loss G: -0.1582\n",
      "Epoch [159/500] Batch 100/133                Loss D: -0.0104,loss G: -0.4717\n",
      "Epoch [160/500] Batch 0/133                Loss D: -0.0080,loss G: -0.5543\n",
      "Epoch [160/500] Batch 100/133                Loss D: -0.0106,loss G: -0.4628\n",
      "Epoch [161/500] Batch 0/133                Loss D: 0.0058,loss G: -0.6408\n",
      "Epoch [161/500] Batch 100/133                Loss D: 0.0073,loss G: 0.1198\n",
      "Epoch [162/500] Batch 0/133                Loss D: 0.0185,loss G: -0.4552\n",
      "Epoch [162/500] Batch 100/133                Loss D: 0.0022,loss G: -0.2680\n",
      "Epoch [163/500] Batch 0/133                Loss D: 0.0102,loss G: -0.2788\n",
      "Epoch [163/500] Batch 100/133                Loss D: 0.0150,loss G: 0.2418\n",
      "Epoch [164/500] Batch 0/133                Loss D: 0.0065,loss G: -0.3009\n",
      "Epoch [164/500] Batch 100/133                Loss D: 0.0030,loss G: -0.3230\n",
      "Epoch [165/500] Batch 0/133                Loss D: 0.0338,loss G: 0.2269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [165/500] Batch 100/133                Loss D: -0.0064,loss G: -1.0962\n",
      "Epoch [166/500] Batch 0/133                Loss D: -0.0033,loss G: -0.7725\n",
      "Epoch [166/500] Batch 100/133                Loss D: -0.0076,loss G: -0.1869\n",
      "Epoch [167/500] Batch 0/133                Loss D: -0.0088,loss G: -0.3502\n",
      "Epoch [167/500] Batch 100/133                Loss D: -0.0047,loss G: -0.1041\n",
      "Epoch [168/500] Batch 0/133                Loss D: 0.0021,loss G: -0.1583\n",
      "Epoch [168/500] Batch 100/133                Loss D: -0.0034,loss G: -1.0901\n",
      "Epoch [169/500] Batch 0/133                Loss D: -0.0048,loss G: -0.7510\n",
      "Epoch [169/500] Batch 100/133                Loss D: -0.0052,loss G: -0.8453\n",
      "Epoch [170/500] Batch 0/133                Loss D: -0.0020,loss G: -0.8779\n",
      "Epoch [170/500] Batch 100/133                Loss D: -0.0031,loss G: -1.0073\n",
      "Epoch [171/500] Batch 0/133                Loss D: -0.0075,loss G: -1.2807\n",
      "Epoch [171/500] Batch 100/133                Loss D: -0.0075,loss G: -1.0151\n",
      "Epoch [172/500] Batch 0/133                Loss D: -0.0040,loss G: -0.7893\n",
      "Epoch [172/500] Batch 100/133                Loss D: -0.0036,loss G: -0.2605\n",
      "Epoch [173/500] Batch 0/133                Loss D: -0.0065,loss G: -0.3961\n",
      "Epoch [173/500] Batch 100/133                Loss D: -0.0064,loss G: -0.3207\n",
      "Epoch [174/500] Batch 0/133                Loss D: 0.0105,loss G: -0.6634\n",
      "Epoch [174/500] Batch 100/133                Loss D: 0.0099,loss G: -1.2142\n",
      "Epoch [175/500] Batch 0/133                Loss D: 0.0097,loss G: -1.0117\n",
      "Epoch [175/500] Batch 100/133                Loss D: -0.0012,loss G: -1.2431\n",
      "Epoch [176/500] Batch 0/133                Loss D: -0.0016,loss G: -1.3533\n",
      "Epoch [176/500] Batch 100/133                Loss D: -0.0003,loss G: 0.2185\n",
      "Epoch [177/500] Batch 0/133                Loss D: -0.0054,loss G: 0.4313\n",
      "Epoch [177/500] Batch 100/133                Loss D: 0.0002,loss G: 0.3803\n",
      "Epoch [178/500] Batch 0/133                Loss D: 0.0015,loss G: 0.6643\n",
      "Epoch [178/500] Batch 100/133                Loss D: -0.0064,loss G: 0.7951\n",
      "Epoch [179/500] Batch 0/133                Loss D: -0.0014,loss G: 0.5782\n",
      "Epoch [179/500] Batch 100/133                Loss D: -0.0060,loss G: 0.2440\n",
      "Epoch [180/500] Batch 0/133                Loss D: -0.0076,loss G: 0.0201\n",
      "Epoch [180/500] Batch 100/133                Loss D: 0.0077,loss G: 0.0392\n",
      "Epoch [181/500] Batch 0/133                Loss D: 0.0087,loss G: 0.2598\n",
      "Epoch [181/500] Batch 100/133                Loss D: 0.0013,loss G: -0.0220\n",
      "Epoch [182/500] Batch 0/133                Loss D: -0.0011,loss G: 0.0916\n",
      "Epoch [182/500] Batch 100/133                Loss D: 0.0005,loss G: -0.1804\n",
      "Epoch [183/500] Batch 0/133                Loss D: -0.0051,loss G: -0.0192\n",
      "Epoch [183/500] Batch 100/133                Loss D: -0.0039,loss G: 0.4147\n",
      "Epoch [184/500] Batch 0/133                Loss D: -0.0059,loss G: 0.1199\n",
      "Epoch [184/500] Batch 100/133                Loss D: -0.0051,loss G: 0.3133\n",
      "Epoch [185/500] Batch 0/133                Loss D: -0.0047,loss G: 0.7137\n",
      "Epoch [185/500] Batch 100/133                Loss D: -0.0087,loss G: 0.2862\n",
      "Epoch [186/500] Batch 0/133                Loss D: -0.0022,loss G: 0.1920\n",
      "Epoch [186/500] Batch 100/133                Loss D: -0.0036,loss G: -0.0669\n",
      "Epoch [187/500] Batch 0/133                Loss D: 0.0036,loss G: -0.4067\n",
      "Epoch [187/500] Batch 100/133                Loss D: -0.0037,loss G: 0.3238\n",
      "Epoch [188/500] Batch 0/133                Loss D: -0.0058,loss G: 0.3146\n",
      "Epoch [188/500] Batch 100/133                Loss D: -0.0026,loss G: -0.3585\n",
      "Epoch [189/500] Batch 0/133                Loss D: -0.0010,loss G: -0.5609\n",
      "Epoch [189/500] Batch 100/133                Loss D: -0.0047,loss G: -1.1921\n",
      "Epoch [190/500] Batch 0/133                Loss D: -0.0044,loss G: -1.2205\n",
      "Epoch [190/500] Batch 100/133                Loss D: -0.0059,loss G: -0.8830\n",
      "Epoch [191/500] Batch 0/133                Loss D: -0.0044,loss G: -1.0545\n",
      "Epoch [191/500] Batch 100/133                Loss D: -0.0064,loss G: -1.0737\n",
      "Epoch [192/500] Batch 0/133                Loss D: 0.0006,loss G: -1.1570\n",
      "Epoch [192/500] Batch 100/133                Loss D: -0.0032,loss G: -1.0310\n",
      "Epoch [193/500] Batch 0/133                Loss D: -0.0008,loss G: -1.0957\n",
      "Epoch [193/500] Batch 100/133                Loss D: -0.0071,loss G: -1.0517\n",
      "Epoch [194/500] Batch 0/133                Loss D: 0.0046,loss G: -1.3304\n",
      "Epoch [194/500] Batch 100/133                Loss D: -0.0032,loss G: -1.7853\n",
      "Epoch [195/500] Batch 0/133                Loss D: 0.0018,loss G: -1.6869\n",
      "Epoch [195/500] Batch 100/133                Loss D: -0.0043,loss G: -1.4060\n",
      "Epoch [196/500] Batch 0/133                Loss D: -0.0048,loss G: -1.4838\n",
      "Epoch [196/500] Batch 100/133                Loss D: -0.0051,loss G: -1.5339\n",
      "Epoch [197/500] Batch 0/133                Loss D: 0.0056,loss G: -1.4943\n",
      "Epoch [197/500] Batch 100/133                Loss D: -0.0043,loss G: -1.6811\n",
      "Epoch [198/500] Batch 0/133                Loss D: -0.0006,loss G: -1.5610\n",
      "Epoch [198/500] Batch 100/133                Loss D: 0.0083,loss G: -1.7330\n",
      "Epoch [199/500] Batch 0/133                Loss D: -0.0067,loss G: -1.5407\n",
      "Epoch [199/500] Batch 100/133                Loss D: -0.0042,loss G: -1.2854\n",
      "Epoch [200/500] Batch 0/133                Loss D: -0.0079,loss G: -0.9166\n",
      "Epoch [200/500] Batch 100/133                Loss D: -0.0016,loss G: -1.6001\n",
      "Epoch [201/500] Batch 0/133                Loss D: -0.0070,loss G: -0.8934\n",
      "Epoch [201/500] Batch 100/133                Loss D: -0.0064,loss G: -1.1900\n",
      "Epoch [202/500] Batch 0/133                Loss D: -0.0029,loss G: -0.9783\n",
      "Epoch [202/500] Batch 100/133                Loss D: -0.0090,loss G: -1.0702\n",
      "Epoch [203/500] Batch 0/133                Loss D: -0.0125,loss G: -1.1645\n",
      "Epoch [203/500] Batch 100/133                Loss D: 0.0012,loss G: -1.0786\n",
      "Epoch [204/500] Batch 0/133                Loss D: 0.0061,loss G: -1.7535\n",
      "Epoch [204/500] Batch 100/133                Loss D: 0.0014,loss G: -4.0766\n",
      "Epoch [205/500] Batch 0/133                Loss D: 0.0186,loss G: -3.8852\n",
      "Epoch [205/500] Batch 100/133                Loss D: -0.0004,loss G: -3.7210\n",
      "Epoch [206/500] Batch 0/133                Loss D: -0.0043,loss G: -4.3510\n",
      "Epoch [206/500] Batch 100/133                Loss D: -0.0076,loss G: -3.7203\n",
      "Epoch [207/500] Batch 0/133                Loss D: -0.0088,loss G: -3.3733\n",
      "Epoch [207/500] Batch 100/133                Loss D: -0.0142,loss G: -4.2878\n",
      "Epoch [208/500] Batch 0/133                Loss D: -0.0065,loss G: -3.9258\n",
      "Epoch [208/500] Batch 100/133                Loss D: -0.0143,loss G: -3.7371\n",
      "Epoch [209/500] Batch 0/133                Loss D: -0.0170,loss G: -4.8892\n",
      "Epoch [209/500] Batch 100/133                Loss D: -0.0141,loss G: -3.5624\n",
      "Epoch [210/500] Batch 0/133                Loss D: -0.0057,loss G: -3.8390\n",
      "Epoch [210/500] Batch 100/133                Loss D: 0.0038,loss G: -3.3781\n",
      "Epoch [211/500] Batch 0/133                Loss D: 0.0152,loss G: -2.8596\n",
      "Epoch [211/500] Batch 100/133                Loss D: -0.0019,loss G: -2.5477\n",
      "Epoch [212/500] Batch 0/133                Loss D: -0.0024,loss G: -3.1162\n",
      "Epoch [212/500] Batch 100/133                Loss D: -0.0064,loss G: -2.3537\n",
      "Epoch [213/500] Batch 0/133                Loss D: -0.0078,loss G: -1.7829\n",
      "Epoch [213/500] Batch 100/133                Loss D: -0.0119,loss G: -2.2528\n",
      "Epoch [214/500] Batch 0/133                Loss D: -0.0110,loss G: -1.6414\n",
      "Epoch [214/500] Batch 100/133                Loss D: -0.0139,loss G: -2.4652\n",
      "Epoch [215/500] Batch 0/133                Loss D: -0.0173,loss G: -1.3122\n",
      "Epoch [215/500] Batch 100/133                Loss D: -0.0123,loss G: -2.4842\n",
      "Epoch [216/500] Batch 0/133                Loss D: 0.0056,loss G: -2.7908\n",
      "Epoch [216/500] Batch 100/133                Loss D: -0.0025,loss G: -2.0831\n",
      "Epoch [217/500] Batch 0/133                Loss D: -0.0070,loss G: -2.1267\n",
      "Epoch [217/500] Batch 100/133                Loss D: -0.0080,loss G: -1.6076\n",
      "Epoch [218/500] Batch 0/133                Loss D: -0.0066,loss G: -1.5981\n",
      "Epoch [218/500] Batch 100/133                Loss D: -0.0074,loss G: -1.7540\n",
      "Epoch [219/500] Batch 0/133                Loss D: -0.0050,loss G: -1.5937\n",
      "Epoch [219/500] Batch 100/133                Loss D: -0.0023,loss G: -2.8661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [220/500] Batch 0/133                Loss D: -0.0106,loss G: -3.6371\n",
      "Epoch [220/500] Batch 100/133                Loss D: -0.0073,loss G: -3.5918\n",
      "Epoch [221/500] Batch 0/133                Loss D: 0.0046,loss G: -3.9391\n",
      "Epoch [221/500] Batch 100/133                Loss D: -0.0100,loss G: -3.1780\n",
      "Epoch [222/500] Batch 0/133                Loss D: -0.0088,loss G: -3.0397\n",
      "Epoch [222/500] Batch 100/133                Loss D: -0.0064,loss G: -2.7613\n",
      "Epoch [223/500] Batch 0/133                Loss D: -0.0041,loss G: -3.0668\n",
      "Epoch [223/500] Batch 100/133                Loss D: -0.0071,loss G: -2.5560\n",
      "Epoch [224/500] Batch 0/133                Loss D: -0.0081,loss G: -2.4527\n",
      "Epoch [224/500] Batch 100/133                Loss D: -0.0067,loss G: -2.0973\n",
      "Epoch [225/500] Batch 0/133                Loss D: -0.0137,loss G: -2.2754\n",
      "Epoch [225/500] Batch 100/133                Loss D: -0.0034,loss G: -3.0881\n",
      "Epoch [226/500] Batch 0/133                Loss D: -0.0078,loss G: -3.0962\n",
      "Epoch [226/500] Batch 100/133                Loss D: -0.0106,loss G: -2.9354\n",
      "Epoch [227/500] Batch 0/133                Loss D: -0.0043,loss G: -3.1184\n",
      "Epoch [227/500] Batch 100/133                Loss D: -0.0117,loss G: -2.6923\n",
      "Epoch [228/500] Batch 0/133                Loss D: -0.0084,loss G: -1.8136\n",
      "Epoch [228/500] Batch 100/133                Loss D: 0.0048,loss G: -1.5840\n",
      "Epoch [229/500] Batch 0/133                Loss D: -0.0086,loss G: -0.6193\n",
      "Epoch [229/500] Batch 100/133                Loss D: -0.0012,loss G: -1.1324\n",
      "Epoch [230/500] Batch 0/133                Loss D: -0.0025,loss G: -1.4796\n",
      "Epoch [230/500] Batch 100/133                Loss D: -0.0029,loss G: -1.7243\n",
      "Epoch [231/500] Batch 0/133                Loss D: -0.0047,loss G: -1.5179\n",
      "Epoch [231/500] Batch 100/133                Loss D: -0.0089,loss G: -1.2508\n",
      "Epoch [232/500] Batch 0/133                Loss D: -0.0094,loss G: -1.0867\n",
      "Epoch [232/500] Batch 100/133                Loss D: -0.0026,loss G: -0.6267\n",
      "Epoch [233/500] Batch 0/133                Loss D: 0.0153,loss G: -0.8828\n",
      "Epoch [233/500] Batch 100/133                Loss D: -0.0078,loss G: -0.5519\n",
      "Epoch [234/500] Batch 0/133                Loss D: -0.0087,loss G: -0.8711\n",
      "Epoch [234/500] Batch 100/133                Loss D: -0.0007,loss G: 0.1339\n",
      "Epoch [235/500] Batch 0/133                Loss D: -0.0018,loss G: 1.6109\n",
      "Epoch [235/500] Batch 100/133                Loss D: -0.0037,loss G: 0.6596\n",
      "Epoch [236/500] Batch 0/133                Loss D: -0.0014,loss G: 0.7402\n",
      "Epoch [236/500] Batch 100/133                Loss D: -0.0031,loss G: 1.1072\n",
      "Epoch [237/500] Batch 0/133                Loss D: 0.0093,loss G: 0.7108\n",
      "Epoch [237/500] Batch 100/133                Loss D: -0.0016,loss G: 0.7471\n",
      "Epoch [238/500] Batch 0/133                Loss D: 0.0001,loss G: -0.4371\n",
      "Epoch [238/500] Batch 100/133                Loss D: -0.0087,loss G: 0.2634\n",
      "Epoch [239/500] Batch 0/133                Loss D: -0.0067,loss G: -0.0357\n",
      "Epoch [239/500] Batch 100/133                Loss D: 0.0025,loss G: -0.1903\n",
      "Epoch [240/500] Batch 0/133                Loss D: 0.0018,loss G: -0.4133\n",
      "Epoch [240/500] Batch 100/133                Loss D: -0.0024,loss G: -0.7338\n",
      "Epoch [241/500] Batch 0/133                Loss D: -0.0045,loss G: -0.6742\n",
      "Epoch [241/500] Batch 100/133                Loss D: -0.0056,loss G: -0.6273\n",
      "Epoch [242/500] Batch 0/133                Loss D: -0.0013,loss G: -0.2015\n",
      "Epoch [242/500] Batch 100/133                Loss D: 0.0267,loss G: -0.7525\n",
      "Epoch [243/500] Batch 0/133                Loss D: -0.0022,loss G: -0.4007\n",
      "Epoch [243/500] Batch 100/133                Loss D: -0.0089,loss G: -0.0490\n",
      "Epoch [244/500] Batch 0/133                Loss D: -0.0056,loss G: -0.1919\n",
      "Epoch [244/500] Batch 100/133                Loss D: -0.0032,loss G: -0.7466\n",
      "Epoch [245/500] Batch 0/133                Loss D: -0.0060,loss G: -0.6113\n",
      "Epoch [245/500] Batch 100/133                Loss D: -0.0070,loss G: -1.0875\n",
      "Epoch [246/500] Batch 0/133                Loss D: 0.0250,loss G: -0.4321\n",
      "Epoch [246/500] Batch 100/133                Loss D: -0.0009,loss G: -0.2318\n",
      "Epoch [247/500] Batch 0/133                Loss D: -0.0063,loss G: 0.0606\n",
      "Epoch [247/500] Batch 100/133                Loss D: -0.0037,loss G: -1.1664\n",
      "Epoch [248/500] Batch 0/133                Loss D: -0.0038,loss G: -1.5252\n",
      "Epoch [248/500] Batch 100/133                Loss D: -0.0014,loss G: -1.1759\n",
      "Epoch [249/500] Batch 0/133                Loss D: -0.0098,loss G: -1.0684\n",
      "Epoch [249/500] Batch 100/133                Loss D: -0.0116,loss G: -0.5574\n",
      "Epoch [250/500] Batch 0/133                Loss D: 0.0318,loss G: -0.6175\n",
      "Epoch [250/500] Batch 100/133                Loss D: -0.0065,loss G: -1.0996\n",
      "Epoch [251/500] Batch 0/133                Loss D: -0.0010,loss G: -1.2673\n",
      "Epoch [251/500] Batch 100/133                Loss D: -0.0072,loss G: -1.2862\n",
      "Epoch [252/500] Batch 0/133                Loss D: -0.0005,loss G: -1.8926\n",
      "Epoch [252/500] Batch 100/133                Loss D: 0.0300,loss G: -1.2422\n",
      "Epoch [253/500] Batch 0/133                Loss D: -0.0054,loss G: -1.2419\n",
      "Epoch [253/500] Batch 100/133                Loss D: -0.0014,loss G: -1.0682\n",
      "Epoch [254/500] Batch 0/133                Loss D: -0.0031,loss G: -1.0243\n",
      "Epoch [254/500] Batch 100/133                Loss D: -0.0017,loss G: -1.2131\n",
      "Epoch [255/500] Batch 0/133                Loss D: -0.0037,loss G: -1.5917\n",
      "Epoch [255/500] Batch 100/133                Loss D: -0.0038,loss G: -0.7411\n",
      "Epoch [256/500] Batch 0/133                Loss D: -0.0042,loss G: -0.8228\n",
      "Epoch [256/500] Batch 100/133                Loss D: -0.0048,loss G: -1.2118\n",
      "Epoch [257/500] Batch 0/133                Loss D: 0.0102,loss G: -1.1378\n",
      "Epoch [257/500] Batch 100/133                Loss D: 0.0003,loss G: -0.8481\n",
      "Epoch [258/500] Batch 0/133                Loss D: -0.0030,loss G: -1.2273\n",
      "Epoch [258/500] Batch 100/133                Loss D: -0.0022,loss G: -1.4806\n",
      "Epoch [259/500] Batch 0/133                Loss D: 0.0024,loss G: -1.2473\n",
      "Epoch [259/500] Batch 100/133                Loss D: -0.0036,loss G: -1.0174\n",
      "Epoch [260/500] Batch 0/133                Loss D: -0.0005,loss G: -0.7305\n",
      "Epoch [260/500] Batch 100/133                Loss D: -0.0014,loss G: -1.1900\n",
      "Epoch [261/500] Batch 0/133                Loss D: 0.0040,loss G: -0.8499\n",
      "Epoch [261/500] Batch 100/133                Loss D: -0.0049,loss G: 0.1854\n",
      "Epoch [262/500] Batch 0/133                Loss D: -0.0012,loss G: -0.2409\n",
      "Epoch [262/500] Batch 100/133                Loss D: -0.0071,loss G: -0.3233\n",
      "Epoch [263/500] Batch 0/133                Loss D: -0.0081,loss G: -0.3029\n",
      "Epoch [263/500] Batch 100/133                Loss D: -0.0019,loss G: -1.0483\n",
      "Epoch [264/500] Batch 0/133                Loss D: -0.0056,loss G: -2.0355\n",
      "Epoch [264/500] Batch 100/133                Loss D: -0.0002,loss G: -1.9858\n",
      "Epoch [265/500] Batch 0/133                Loss D: -0.0046,loss G: -2.0764\n",
      "Epoch [265/500] Batch 100/133                Loss D: -0.0064,loss G: -1.8280\n",
      "Epoch [266/500] Batch 0/133                Loss D: 0.0099,loss G: -1.8256\n",
      "Epoch [266/500] Batch 100/133                Loss D: -0.0009,loss G: -1.6229\n",
      "Epoch [267/500] Batch 0/133                Loss D: -0.0005,loss G: -2.1379\n",
      "Epoch [267/500] Batch 100/133                Loss D: -0.0069,loss G: -1.6664\n",
      "Epoch [268/500] Batch 0/133                Loss D: 0.0039,loss G: -1.2982\n",
      "Epoch [268/500] Batch 100/133                Loss D: -0.0081,loss G: -1.1396\n",
      "Epoch [269/500] Batch 0/133                Loss D: 0.0123,loss G: -1.0008\n",
      "Epoch [269/500] Batch 100/133                Loss D: -0.0109,loss G: -0.9920\n",
      "Epoch [270/500] Batch 0/133                Loss D: -0.0101,loss G: -1.3069\n",
      "Epoch [270/500] Batch 100/133                Loss D: -0.0141,loss G: -0.6554\n",
      "Epoch [271/500] Batch 0/133                Loss D: -0.0102,loss G: -0.6665\n",
      "Epoch [271/500] Batch 100/133                Loss D: 0.0053,loss G: -4.6100\n",
      "Epoch [272/500] Batch 0/133                Loss D: 0.0104,loss G: -4.0277\n",
      "Epoch [272/500] Batch 100/133                Loss D: -0.0082,loss G: -3.7542\n",
      "Epoch [273/500] Batch 0/133                Loss D: 0.0095,loss G: -3.7270\n",
      "Epoch [273/500] Batch 100/133                Loss D: 0.0014,loss G: -2.7876\n",
      "Epoch [274/500] Batch 0/133                Loss D: -0.0045,loss G: -2.8662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [274/500] Batch 100/133                Loss D: -0.0118,loss G: -2.1343\n",
      "Epoch [275/500] Batch 0/133                Loss D: -0.0008,loss G: -1.8102\n",
      "Epoch [275/500] Batch 100/133                Loss D: -0.0161,loss G: -1.7952\n",
      "Epoch [276/500] Batch 0/133                Loss D: -0.0143,loss G: -1.0900\n",
      "Epoch [276/500] Batch 100/133                Loss D: -0.0157,loss G: -1.6663\n",
      "Epoch [277/500] Batch 0/133                Loss D: -0.0129,loss G: -1.4826\n",
      "Epoch [277/500] Batch 100/133                Loss D: 0.0074,loss G: -3.7842\n",
      "Epoch [278/500] Batch 0/133                Loss D: -0.0034,loss G: -3.3667\n",
      "Epoch [278/500] Batch 100/133                Loss D: -0.0024,loss G: -3.9969\n",
      "Epoch [279/500] Batch 0/133                Loss D: -0.0055,loss G: -4.2577\n",
      "Epoch [279/500] Batch 100/133                Loss D: -0.0076,loss G: -3.8463\n",
      "Epoch [280/500] Batch 0/133                Loss D: -0.0070,loss G: -3.7192\n",
      "Epoch [280/500] Batch 100/133                Loss D: -0.0106,loss G: -3.1218\n",
      "Epoch [281/500] Batch 0/133                Loss D: -0.0053,loss G: -2.6929\n",
      "Epoch [281/500] Batch 100/133                Loss D: -0.0082,loss G: -2.6656\n",
      "Epoch [282/500] Batch 0/133                Loss D: -0.0049,loss G: -2.7762\n",
      "Epoch [282/500] Batch 100/133                Loss D: -0.0033,loss G: -2.3389\n",
      "Epoch [283/500] Batch 0/133                Loss D: -0.0088,loss G: -2.2163\n",
      "Epoch [283/500] Batch 100/133                Loss D: -0.0086,loss G: -2.3003\n",
      "Epoch [284/500] Batch 0/133                Loss D: 0.0028,loss G: -2.6529\n",
      "Epoch [284/500] Batch 100/133                Loss D: -0.0030,loss G: -2.3636\n",
      "Epoch [285/500] Batch 0/133                Loss D: -0.0006,loss G: -2.9883\n",
      "Epoch [285/500] Batch 100/133                Loss D: -0.0095,loss G: -2.6132\n",
      "Epoch [286/500] Batch 0/133                Loss D: -0.0008,loss G: -2.6570\n",
      "Epoch [286/500] Batch 100/133                Loss D: -0.0095,loss G: -2.8801\n",
      "Epoch [287/500] Batch 0/133                Loss D: -0.0051,loss G: -2.8821\n",
      "Epoch [287/500] Batch 100/133                Loss D: 0.0064,loss G: -1.6470\n",
      "Epoch [288/500] Batch 0/133                Loss D: -0.0065,loss G: -1.3707\n",
      "Epoch [288/500] Batch 100/133                Loss D: -0.0079,loss G: -2.7703\n",
      "Epoch [289/500] Batch 0/133                Loss D: -0.0124,loss G: -2.8735\n",
      "Epoch [289/500] Batch 100/133                Loss D: -0.0085,loss G: -2.7623\n",
      "Epoch [290/500] Batch 0/133                Loss D: -0.0053,loss G: -2.6387\n",
      "Epoch [290/500] Batch 100/133                Loss D: 0.0019,loss G: -1.7924\n",
      "Epoch [291/500] Batch 0/133                Loss D: -0.0038,loss G: -1.1466\n",
      "Epoch [291/500] Batch 100/133                Loss D: -0.0070,loss G: -1.2369\n",
      "Epoch [292/500] Batch 0/133                Loss D: -0.0009,loss G: -1.2000\n",
      "Epoch [292/500] Batch 100/133                Loss D: -0.0046,loss G: -2.1982\n",
      "Epoch [293/500] Batch 0/133                Loss D: -0.0016,loss G: -2.1890\n",
      "Epoch [293/500] Batch 100/133                Loss D: 0.0054,loss G: -2.6496\n",
      "Epoch [294/500] Batch 0/133                Loss D: -0.0015,loss G: -2.6064\n",
      "Epoch [294/500] Batch 100/133                Loss D: -0.0052,loss G: -2.5752\n",
      "Epoch [295/500] Batch 0/133                Loss D: -0.0050,loss G: -2.2422\n",
      "Epoch [295/500] Batch 100/133                Loss D: -0.0042,loss G: -2.1889\n",
      "Epoch [296/500] Batch 0/133                Loss D: -0.0039,loss G: -2.2739\n",
      "Epoch [296/500] Batch 100/133                Loss D: -0.0133,loss G: -1.4762\n",
      "Epoch [297/500] Batch 0/133                Loss D: 0.0015,loss G: -0.9489\n",
      "Epoch [297/500] Batch 100/133                Loss D: 0.0021,loss G: -1.3054\n",
      "Epoch [298/500] Batch 0/133                Loss D: -0.0041,loss G: -1.2109\n",
      "Epoch [298/500] Batch 100/133                Loss D: -0.0069,loss G: -2.3620\n",
      "Epoch [299/500] Batch 0/133                Loss D: -0.0055,loss G: -2.3446\n",
      "Epoch [299/500] Batch 100/133                Loss D: 0.0026,loss G: -3.0243\n",
      "Epoch [300/500] Batch 0/133                Loss D: -0.0045,loss G: -3.0670\n",
      "Epoch [300/500] Batch 100/133                Loss D: -0.0093,loss G: -2.6158\n",
      "Epoch [301/500] Batch 0/133                Loss D: -0.0064,loss G: -2.8652\n",
      "Epoch [301/500] Batch 100/133                Loss D: -0.0098,loss G: -2.0331\n",
      "Epoch [302/500] Batch 0/133                Loss D: -0.0051,loss G: -1.9587\n",
      "Epoch [302/500] Batch 100/133                Loss D: -0.0051,loss G: -2.3722\n",
      "Epoch [303/500] Batch 0/133                Loss D: -0.0013,loss G: -1.5469\n",
      "Epoch [303/500] Batch 100/133                Loss D: -0.0122,loss G: -0.9798\n",
      "Epoch [304/500] Batch 0/133                Loss D: -0.0086,loss G: -1.0522\n",
      "Epoch [304/500] Batch 100/133                Loss D: -0.0054,loss G: -0.6295\n",
      "Epoch [305/500] Batch 0/133                Loss D: -0.0093,loss G: -0.1319\n",
      "Epoch [305/500] Batch 100/133                Loss D: -0.0054,loss G: -0.8732\n",
      "Epoch [306/500] Batch 0/133                Loss D: -0.0025,loss G: -0.4532\n",
      "Epoch [306/500] Batch 100/133                Loss D: -0.0077,loss G: -1.4204\n",
      "Epoch [307/500] Batch 0/133                Loss D: -0.0073,loss G: -0.8137\n",
      "Epoch [307/500] Batch 100/133                Loss D: -0.0042,loss G: -1.0667\n",
      "Epoch [308/500] Batch 0/133                Loss D: -0.0052,loss G: -1.5566\n",
      "Epoch [308/500] Batch 100/133                Loss D: -0.0034,loss G: -1.4302\n",
      "Epoch [309/500] Batch 0/133                Loss D: -0.0063,loss G: -1.2776\n",
      "Epoch [309/500] Batch 100/133                Loss D: -0.0045,loss G: -1.1170\n",
      "Epoch [310/500] Batch 0/133                Loss D: -0.0054,loss G: -1.1121\n",
      "Epoch [310/500] Batch 100/133                Loss D: -0.0064,loss G: -0.7953\n",
      "Epoch [311/500] Batch 0/133                Loss D: -0.0055,loss G: -0.9222\n",
      "Epoch [311/500] Batch 100/133                Loss D: -0.0069,loss G: -1.2663\n",
      "Epoch [312/500] Batch 0/133                Loss D: -0.0041,loss G: -1.6519\n",
      "Epoch [312/500] Batch 100/133                Loss D: -0.0007,loss G: -1.9470\n",
      "Epoch [313/500] Batch 0/133                Loss D: -0.0020,loss G: -2.3105\n",
      "Epoch [313/500] Batch 100/133                Loss D: -0.0054,loss G: -2.5163\n",
      "Epoch [314/500] Batch 0/133                Loss D: -0.0043,loss G: -2.2696\n",
      "Epoch [314/500] Batch 100/133                Loss D: -0.0075,loss G: -2.2071\n",
      "Epoch [315/500] Batch 0/133                Loss D: -0.0055,loss G: -1.7681\n",
      "Epoch [315/500] Batch 100/133                Loss D: -0.0043,loss G: -1.6814\n",
      "Epoch [316/500] Batch 0/133                Loss D: -0.0051,loss G: -1.5868\n",
      "Epoch [316/500] Batch 100/133                Loss D: -0.0077,loss G: -1.5269\n",
      "Epoch [317/500] Batch 0/133                Loss D: -0.0009,loss G: -1.6949\n",
      "Epoch [317/500] Batch 100/133                Loss D: -0.0086,loss G: -1.8509\n",
      "Epoch [318/500] Batch 0/133                Loss D: -0.0039,loss G: -1.9345\n",
      "Epoch [318/500] Batch 100/133                Loss D: -0.0023,loss G: -0.9901\n",
      "Epoch [319/500] Batch 0/133                Loss D: -0.0042,loss G: -0.9427\n",
      "Epoch [319/500] Batch 100/133                Loss D: -0.0067,loss G: -1.2108\n",
      "Epoch [320/500] Batch 0/133                Loss D: -0.0069,loss G: -1.1479\n",
      "Epoch [320/500] Batch 100/133                Loss D: -0.0091,loss G: -1.9769\n",
      "Epoch [321/500] Batch 0/133                Loss D: -0.0106,loss G: -1.9529\n",
      "Epoch [321/500] Batch 100/133                Loss D: -0.0107,loss G: -1.1365\n",
      "Epoch [322/500] Batch 0/133                Loss D: -0.0074,loss G: -1.1358\n",
      "Epoch [322/500] Batch 100/133                Loss D: -0.0105,loss G: -0.3823\n",
      "Epoch [323/500] Batch 0/133                Loss D: -0.0071,loss G: 0.5019\n",
      "Epoch [323/500] Batch 100/133                Loss D: -0.0046,loss G: -0.6970\n",
      "Epoch [324/500] Batch 0/133                Loss D: -0.0071,loss G: -1.1005\n",
      "Epoch [324/500] Batch 100/133                Loss D: -0.0048,loss G: -2.5304\n",
      "Epoch [325/500] Batch 0/133                Loss D: -0.0046,loss G: -2.6016\n",
      "Epoch [325/500] Batch 100/133                Loss D: -0.0034,loss G: -2.6680\n",
      "Epoch [326/500] Batch 0/133                Loss D: -0.0083,loss G: -2.4306\n",
      "Epoch [326/500] Batch 100/133                Loss D: 0.0027,loss G: -1.5838\n",
      "Epoch [327/500] Batch 0/133                Loss D: 0.0032,loss G: -1.6404\n",
      "Epoch [327/500] Batch 100/133                Loss D: -0.0059,loss G: -1.5145\n",
      "Epoch [328/500] Batch 0/133                Loss D: -0.0188,loss G: -1.3894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [328/500] Batch 100/133                Loss D: -0.0034,loss G: -1.8861\n",
      "Epoch [329/500] Batch 0/133                Loss D: 0.0056,loss G: -1.9124\n",
      "Epoch [329/500] Batch 100/133                Loss D: -0.0068,loss G: -2.3652\n",
      "Epoch [330/500] Batch 0/133                Loss D: -0.0078,loss G: -2.2362\n",
      "Epoch [330/500] Batch 100/133                Loss D: -0.0166,loss G: -3.2824\n",
      "Epoch [331/500] Batch 0/133                Loss D: -0.0037,loss G: -3.4914\n",
      "Epoch [331/500] Batch 100/133                Loss D: -0.0001,loss G: -3.5184\n",
      "Epoch [332/500] Batch 0/133                Loss D: 0.0012,loss G: -3.6420\n",
      "Epoch [332/500] Batch 100/133                Loss D: -0.0048,loss G: -3.5053\n",
      "Epoch [333/500] Batch 0/133                Loss D: -0.0051,loss G: -3.5411\n",
      "Epoch [333/500] Batch 100/133                Loss D: -0.0078,loss G: -3.1085\n",
      "Epoch [334/500] Batch 0/133                Loss D: -0.0101,loss G: -3.1504\n",
      "Epoch [334/500] Batch 100/133                Loss D: 0.0047,loss G: -3.5725\n",
      "Epoch [335/500] Batch 0/133                Loss D: 0.0063,loss G: -4.0174\n",
      "Epoch [335/500] Batch 100/133                Loss D: 0.0034,loss G: -6.0994\n",
      "Epoch [336/500] Batch 0/133                Loss D: -0.0125,loss G: -5.6344\n",
      "Epoch [336/500] Batch 100/133                Loss D: -0.0081,loss G: -5.4986\n",
      "Epoch [337/500] Batch 0/133                Loss D: -0.0135,loss G: -5.4576\n",
      "Epoch [337/500] Batch 100/133                Loss D: -0.0183,loss G: -6.2733\n",
      "Epoch [338/500] Batch 0/133                Loss D: -0.0296,loss G: -6.1265\n",
      "Epoch [338/500] Batch 100/133                Loss D: -0.0346,loss G: -6.3223\n",
      "Epoch [339/500] Batch 0/133                Loss D: -0.0368,loss G: -7.1057\n",
      "Epoch [339/500] Batch 100/133                Loss D: -0.0339,loss G: -6.5702\n",
      "Epoch [340/500] Batch 0/133                Loss D: -0.0286,loss G: -6.4288\n",
      "Epoch [340/500] Batch 100/133                Loss D: -0.0285,loss G: -5.9730\n",
      "Epoch [341/500] Batch 0/133                Loss D: -0.0238,loss G: -6.0022\n",
      "Epoch [341/500] Batch 100/133                Loss D: -0.0206,loss G: -5.8428\n",
      "Epoch [342/500] Batch 0/133                Loss D: -0.0205,loss G: -5.9122\n",
      "Epoch [342/500] Batch 100/133                Loss D: -0.0198,loss G: -5.7643\n",
      "Epoch [343/500] Batch 0/133                Loss D: -0.0186,loss G: -6.4395\n",
      "Epoch [343/500] Batch 100/133                Loss D: -0.0181,loss G: -6.8632\n",
      "Epoch [344/500] Batch 0/133                Loss D: -0.0169,loss G: -6.9653\n",
      "Epoch [344/500] Batch 100/133                Loss D: -0.0099,loss G: -6.9065\n",
      "Epoch [345/500] Batch 0/133                Loss D: -0.0106,loss G: -6.4143\n",
      "Epoch [345/500] Batch 100/133                Loss D: -0.0114,loss G: -5.3055\n",
      "Epoch [346/500] Batch 0/133                Loss D: -0.0102,loss G: -5.5125\n",
      "Epoch [346/500] Batch 100/133                Loss D: -0.0202,loss G: -5.1401\n",
      "Epoch [347/500] Batch 0/133                Loss D: -0.0182,loss G: -5.4730\n",
      "Epoch [347/500] Batch 100/133                Loss D: -0.0160,loss G: -4.5966\n",
      "Epoch [348/500] Batch 0/133                Loss D: -0.0182,loss G: -4.7353\n",
      "Epoch [348/500] Batch 100/133                Loss D: -0.0197,loss G: -4.6180\n",
      "Epoch [349/500] Batch 0/133                Loss D: -0.0201,loss G: -4.3545\n",
      "Epoch [349/500] Batch 100/133                Loss D: -0.0220,loss G: -4.6607\n",
      "Epoch [350/500] Batch 0/133                Loss D: -0.0181,loss G: -4.1230\n",
      "Epoch [350/500] Batch 100/133                Loss D: -0.0243,loss G: -4.8314\n",
      "Epoch [351/500] Batch 0/133                Loss D: -0.0245,loss G: -5.3672\n",
      "Epoch [351/500] Batch 100/133                Loss D: -0.0213,loss G: -4.9644\n",
      "Epoch [352/500] Batch 0/133                Loss D: -0.0203,loss G: -5.3932\n",
      "Epoch [352/500] Batch 100/133                Loss D: -0.0104,loss G: -4.2026\n",
      "Epoch [353/500] Batch 0/133                Loss D: 0.0361,loss G: -6.4159\n",
      "Epoch [353/500] Batch 100/133                Loss D: 0.0034,loss G: -8.8460\n",
      "Epoch [354/500] Batch 0/133                Loss D: -0.0002,loss G: -9.0217\n",
      "Epoch [354/500] Batch 100/133                Loss D: -0.0055,loss G: -8.9802\n",
      "Epoch [355/500] Batch 0/133                Loss D: -0.0068,loss G: -8.4440\n",
      "Epoch [355/500] Batch 100/133                Loss D: -0.0010,loss G: -7.3581\n",
      "Epoch [356/500] Batch 0/133                Loss D: -0.0041,loss G: -6.7580\n",
      "Epoch [356/500] Batch 100/133                Loss D: -0.0111,loss G: -5.5935\n",
      "Epoch [357/500] Batch 0/133                Loss D: -0.0134,loss G: -5.5554\n",
      "Epoch [357/500] Batch 100/133                Loss D: -0.0100,loss G: -5.2680\n",
      "Epoch [358/500] Batch 0/133                Loss D: -0.0134,loss G: -5.1808\n",
      "Epoch [358/500] Batch 100/133                Loss D: -0.0044,loss G: -5.1230\n",
      "Epoch [359/500] Batch 0/133                Loss D: -0.0092,loss G: -4.6680\n",
      "Epoch [359/500] Batch 100/133                Loss D: -0.0036,loss G: -4.6347\n",
      "Epoch [360/500] Batch 0/133                Loss D: -0.0063,loss G: -4.8079\n",
      "Epoch [360/500] Batch 100/133                Loss D: -0.0081,loss G: -5.4413\n",
      "Epoch [361/500] Batch 0/133                Loss D: -0.0097,loss G: -4.7672\n",
      "Epoch [361/500] Batch 100/133                Loss D: -0.0072,loss G: -4.7770\n",
      "Epoch [362/500] Batch 0/133                Loss D: -0.0093,loss G: -5.1359\n",
      "Epoch [362/500] Batch 100/133                Loss D: -0.0124,loss G: -5.3040\n",
      "Epoch [363/500] Batch 0/133                Loss D: -0.0100,loss G: -5.3780\n",
      "Epoch [363/500] Batch 100/133                Loss D: -0.0060,loss G: -5.0820\n",
      "Epoch [364/500] Batch 0/133                Loss D: -0.0107,loss G: -5.1357\n",
      "Epoch [364/500] Batch 100/133                Loss D: -0.0043,loss G: -4.3078\n",
      "Epoch [365/500] Batch 0/133                Loss D: -0.0007,loss G: -4.0598\n",
      "Epoch [365/500] Batch 100/133                Loss D: -0.0074,loss G: -3.3516\n",
      "Epoch [366/500] Batch 0/133                Loss D: 0.0167,loss G: -3.4170\n",
      "Epoch [366/500] Batch 100/133                Loss D: -0.0100,loss G: -3.3741\n",
      "Epoch [367/500] Batch 0/133                Loss D: -0.0106,loss G: -3.5352\n",
      "Epoch [367/500] Batch 100/133                Loss D: -0.0094,loss G: -2.9719\n",
      "Epoch [368/500] Batch 0/133                Loss D: -0.0038,loss G: -3.0592\n",
      "Epoch [368/500] Batch 100/133                Loss D: -0.0058,loss G: -3.0985\n",
      "Epoch [369/500] Batch 0/133                Loss D: -0.0067,loss G: -3.6062\n",
      "Epoch [369/500] Batch 100/133                Loss D: 0.0056,loss G: -3.8182\n",
      "Epoch [370/500] Batch 0/133                Loss D: -0.0069,loss G: -3.9602\n",
      "Epoch [370/500] Batch 100/133                Loss D: -0.0082,loss G: -3.3975\n",
      "Epoch [371/500] Batch 0/133                Loss D: -0.0095,loss G: -3.3380\n",
      "Epoch [371/500] Batch 100/133                Loss D: -0.0069,loss G: -3.3048\n",
      "Epoch [372/500] Batch 0/133                Loss D: -0.0087,loss G: -3.0652\n",
      "Epoch [372/500] Batch 100/133                Loss D: -0.0077,loss G: -2.8996\n",
      "Epoch [373/500] Batch 0/133                Loss D: -0.0045,loss G: -2.8752\n",
      "Epoch [373/500] Batch 100/133                Loss D: -0.0076,loss G: -3.0922\n",
      "Epoch [374/500] Batch 0/133                Loss D: -0.0044,loss G: -3.0160\n",
      "Epoch [374/500] Batch 100/133                Loss D: -0.0155,loss G: -3.0659\n",
      "Epoch [375/500] Batch 0/133                Loss D: -0.0121,loss G: -2.6370\n",
      "Epoch [375/500] Batch 100/133                Loss D: -0.0030,loss G: -2.1715\n",
      "Epoch [376/500] Batch 0/133                Loss D: -0.0039,loss G: -2.5058\n",
      "Epoch [376/500] Batch 100/133                Loss D: -0.0069,loss G: -1.8911\n",
      "Epoch [377/500] Batch 0/133                Loss D: -0.0065,loss G: -1.9153\n",
      "Epoch [377/500] Batch 100/133                Loss D: -0.0062,loss G: -1.8983\n",
      "Epoch [378/500] Batch 0/133                Loss D: -0.0098,loss G: -1.8944\n",
      "Epoch [378/500] Batch 100/133                Loss D: -0.0115,loss G: -1.8619\n",
      "Epoch [379/500] Batch 0/133                Loss D: -0.0113,loss G: -2.0242\n",
      "Epoch [379/500] Batch 100/133                Loss D: -0.0043,loss G: -2.1628\n",
      "Epoch [380/500] Batch 0/133                Loss D: -0.0139,loss G: -2.8493\n",
      "Epoch [380/500] Batch 100/133                Loss D: 0.0025,loss G: -3.3016\n",
      "Epoch [381/500] Batch 0/133                Loss D: -0.0104,loss G: -2.5398\n",
      "Epoch [381/500] Batch 100/133                Loss D: -0.0101,loss G: -2.8047\n",
      "Epoch [382/500] Batch 0/133                Loss D: 0.0016,loss G: -2.5873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [382/500] Batch 100/133                Loss D: -0.0095,loss G: -2.5058\n",
      "Epoch [383/500] Batch 0/133                Loss D: -0.0118,loss G: -2.4363\n",
      "Epoch [383/500] Batch 100/133                Loss D: -0.0132,loss G: -1.9299\n",
      "Epoch [384/500] Batch 0/133                Loss D: -0.0139,loss G: -2.4736\n",
      "Epoch [384/500] Batch 100/133                Loss D: -0.0073,loss G: -2.3368\n",
      "Epoch [385/500] Batch 0/133                Loss D: -0.0074,loss G: -2.3029\n",
      "Epoch [385/500] Batch 100/133                Loss D: -0.0107,loss G: -2.6086\n",
      "Epoch [386/500] Batch 0/133                Loss D: -0.0027,loss G: -2.6790\n",
      "Epoch [386/500] Batch 100/133                Loss D: -0.0122,loss G: -2.8134\n",
      "Epoch [387/500] Batch 0/133                Loss D: -0.0174,loss G: -2.6078\n",
      "Epoch [387/500] Batch 100/133                Loss D: 0.0097,loss G: -2.4047\n",
      "Epoch [388/500] Batch 0/133                Loss D: -0.0082,loss G: -1.6782\n",
      "Epoch [388/500] Batch 100/133                Loss D: -0.0153,loss G: -1.8091\n",
      "Epoch [389/500] Batch 0/133                Loss D: -0.0155,loss G: -1.6542\n",
      "Epoch [389/500] Batch 100/133                Loss D: 0.0140,loss G: -1.2081\n",
      "Epoch [390/500] Batch 0/133                Loss D: -0.0113,loss G: -0.4963\n",
      "Epoch [390/500] Batch 100/133                Loss D: -0.0135,loss G: -0.5068\n",
      "Epoch [391/500] Batch 0/133                Loss D: -0.0141,loss G: -0.8088\n",
      "Epoch [391/500] Batch 100/133                Loss D: -0.0159,loss G: -0.7549\n",
      "Epoch [392/500] Batch 0/133                Loss D: -0.0067,loss G: -0.1668\n",
      "Epoch [392/500] Batch 100/133                Loss D: -0.0206,loss G: -1.2281\n",
      "Epoch [393/500] Batch 0/133                Loss D: -0.0187,loss G: -0.6685\n",
      "Epoch [393/500] Batch 100/133                Loss D: -0.0175,loss G: -0.5359\n",
      "Epoch [394/500] Batch 0/133                Loss D: -0.0173,loss G: -0.5219\n",
      "Epoch [394/500] Batch 100/133                Loss D: 0.0053,loss G: -0.5710\n",
      "Epoch [395/500] Batch 0/133                Loss D: 0.0088,loss G: 0.4714\n",
      "Epoch [395/500] Batch 100/133                Loss D: -0.0111,loss G: 0.6323\n",
      "Epoch [396/500] Batch 0/133                Loss D: -0.0091,loss G: 0.4123\n",
      "Epoch [396/500] Batch 100/133                Loss D: -0.0215,loss G: 0.5390\n",
      "Epoch [397/500] Batch 0/133                Loss D: -0.0191,loss G: 0.3706\n",
      "Epoch [397/500] Batch 100/133                Loss D: -0.0266,loss G: 0.1333\n",
      "Epoch [398/500] Batch 0/133                Loss D: -0.0285,loss G: -0.4753\n",
      "Epoch [398/500] Batch 100/133                Loss D: -0.0266,loss G: -0.2714\n",
      "Epoch [399/500] Batch 0/133                Loss D: -0.0257,loss G: -0.5473\n",
      "Epoch [399/500] Batch 100/133                Loss D: -0.0240,loss G: -0.7778\n",
      "Epoch [400/500] Batch 0/133                Loss D: -0.0301,loss G: -0.3981\n",
      "Epoch [400/500] Batch 100/133                Loss D: -0.0248,loss G: -0.4227\n",
      "Epoch [401/500] Batch 0/133                Loss D: -0.0279,loss G: -0.4331\n",
      "Epoch [401/500] Batch 100/133                Loss D: -0.0263,loss G: -0.4302\n",
      "Epoch [402/500] Batch 0/133                Loss D: -0.0270,loss G: -0.6715\n",
      "Epoch [402/500] Batch 100/133                Loss D: -0.0284,loss G: -0.4331\n",
      "Epoch [403/500] Batch 0/133                Loss D: 0.0055,loss G: 2.4254\n",
      "Epoch [403/500] Batch 100/133                Loss D: 0.0151,loss G: 3.6743\n",
      "Epoch [404/500] Batch 0/133                Loss D: 0.0248,loss G: 3.7311\n",
      "Epoch [404/500] Batch 100/133                Loss D: -0.0068,loss G: 1.1631\n",
      "Epoch [405/500] Batch 0/133                Loss D: 0.0085,loss G: 1.0788\n",
      "Epoch [405/500] Batch 100/133                Loss D: -0.0082,loss G: 0.6018\n",
      "Epoch [406/500] Batch 0/133                Loss D: -0.0097,loss G: 0.5596\n",
      "Epoch [406/500] Batch 100/133                Loss D: -0.0053,loss G: 0.2018\n",
      "Epoch [407/500] Batch 0/133                Loss D: -0.0076,loss G: 0.4446\n",
      "Epoch [407/500] Batch 100/133                Loss D: -0.0118,loss G: 0.1366\n",
      "Epoch [408/500] Batch 0/133                Loss D: -0.0086,loss G: -0.1287\n",
      "Epoch [408/500] Batch 100/133                Loss D: -0.0004,loss G: 0.0900\n",
      "Epoch [409/500] Batch 0/133                Loss D: -0.0068,loss G: -0.7102\n",
      "Epoch [409/500] Batch 100/133                Loss D: -0.0157,loss G: 0.7942\n",
      "Epoch [410/500] Batch 0/133                Loss D: -0.0141,loss G: 0.8039\n",
      "Epoch [410/500] Batch 100/133                Loss D: -0.0106,loss G: 1.3415\n",
      "Epoch [411/500] Batch 0/133                Loss D: -0.0179,loss G: 1.0873\n",
      "Epoch [411/500] Batch 100/133                Loss D: -0.0227,loss G: 0.9688\n",
      "Epoch [412/500] Batch 0/133                Loss D: -0.0103,loss G: 0.6854\n",
      "Epoch [412/500] Batch 100/133                Loss D: -0.0154,loss G: 1.0487\n",
      "Epoch [413/500] Batch 0/133                Loss D: -0.0177,loss G: 1.3897\n",
      "Epoch [413/500] Batch 100/133                Loss D: -0.0160,loss G: 1.3209\n",
      "Epoch [414/500] Batch 0/133                Loss D: -0.0206,loss G: 0.9402\n",
      "Epoch [414/500] Batch 100/133                Loss D: -0.0036,loss G: 0.5876\n",
      "Epoch [415/500] Batch 0/133                Loss D: 0.0194,loss G: -0.4065\n",
      "Epoch [415/500] Batch 100/133                Loss D: -0.0024,loss G: -1.5274\n",
      "Epoch [416/500] Batch 0/133                Loss D: -0.0011,loss G: -0.9487\n",
      "Epoch [416/500] Batch 100/133                Loss D: -0.0134,loss G: -1.2541\n",
      "Epoch [417/500] Batch 0/133                Loss D: 0.0101,loss G: -0.6777\n",
      "Epoch [417/500] Batch 100/133                Loss D: -0.0244,loss G: -1.6015\n",
      "Epoch [418/500] Batch 0/133                Loss D: -0.0022,loss G: -1.2364\n",
      "Epoch [418/500] Batch 100/133                Loss D: -0.0076,loss G: -1.7957\n",
      "Epoch [419/500] Batch 0/133                Loss D: -0.0116,loss G: -1.7630\n",
      "Epoch [419/500] Batch 100/133                Loss D: -0.0106,loss G: -1.8610\n",
      "Epoch [420/500] Batch 0/133                Loss D: -0.0097,loss G: -2.2884\n",
      "Epoch [420/500] Batch 100/133                Loss D: -0.0005,loss G: -2.9107\n",
      "Epoch [421/500] Batch 0/133                Loss D: 0.0148,loss G: -4.0388\n",
      "Epoch [421/500] Batch 100/133                Loss D: -0.0165,loss G: -4.0542\n",
      "Epoch [422/500] Batch 0/133                Loss D: -0.0093,loss G: -3.6551\n",
      "Epoch [422/500] Batch 100/133                Loss D: -0.0108,loss G: -3.3629\n",
      "Epoch [423/500] Batch 0/133                Loss D: -0.0112,loss G: -2.8553\n",
      "Epoch [423/500] Batch 100/133                Loss D: -0.0092,loss G: -2.2907\n",
      "Epoch [424/500] Batch 0/133                Loss D: -0.0135,loss G: -1.9865\n",
      "Epoch [424/500] Batch 100/133                Loss D: -0.0330,loss G: -1.7147\n",
      "Epoch [425/500] Batch 0/133                Loss D: -0.0099,loss G: -1.5160\n",
      "Epoch [425/500] Batch 100/133                Loss D: -0.0186,loss G: -1.8950\n",
      "Epoch [426/500] Batch 0/133                Loss D: -0.0214,loss G: -2.0406\n",
      "Epoch [426/500] Batch 100/133                Loss D: -0.0213,loss G: -2.4409\n",
      "Epoch [427/500] Batch 0/133                Loss D: -0.0216,loss G: -2.2463\n",
      "Epoch [427/500] Batch 100/133                Loss D: -0.0156,loss G: -2.5025\n",
      "Epoch [428/500] Batch 0/133                Loss D: -0.0200,loss G: -2.7868\n",
      "Epoch [428/500] Batch 100/133                Loss D: -0.0217,loss G: -1.9542\n",
      "Epoch [429/500] Batch 0/133                Loss D: -0.0210,loss G: -2.1884\n",
      "Epoch [429/500] Batch 100/133                Loss D: -0.0184,loss G: -1.4949\n",
      "Epoch [430/500] Batch 0/133                Loss D: -0.0143,loss G: 0.4910\n",
      "Epoch [430/500] Batch 100/133                Loss D: -0.0157,loss G: 0.2339\n",
      "Epoch [431/500] Batch 0/133                Loss D: -0.0186,loss G: 0.1848\n",
      "Epoch [431/500] Batch 100/133                Loss D: -0.0134,loss G: -0.0025\n",
      "Epoch [432/500] Batch 0/133                Loss D: -0.0155,loss G: -0.2088\n",
      "Epoch [432/500] Batch 100/133                Loss D: -0.0200,loss G: -1.0135\n",
      "Epoch [433/500] Batch 0/133                Loss D: -0.0238,loss G: -1.2739\n",
      "Epoch [433/500] Batch 100/133                Loss D: -0.0283,loss G: -0.6100\n",
      "Epoch [434/500] Batch 0/133                Loss D: -0.0269,loss G: -0.5459\n",
      "Epoch [434/500] Batch 100/133                Loss D: -0.0336,loss G: -1.1423\n",
      "Epoch [435/500] Batch 0/133                Loss D: -0.0271,loss G: -0.9798\n",
      "Epoch [435/500] Batch 100/133                Loss D: -0.0312,loss G: -0.6563\n",
      "Epoch [436/500] Batch 0/133                Loss D: 0.0199,loss G: -1.7562\n",
      "Epoch [436/500] Batch 100/133                Loss D: -0.0112,loss G: -1.3444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [437/500] Batch 0/133                Loss D: -0.0055,loss G: -0.7065\n",
      "Epoch [437/500] Batch 100/133                Loss D: -0.0042,loss G: -0.7224\n",
      "Epoch [438/500] Batch 0/133                Loss D: -0.0049,loss G: -0.4405\n",
      "Epoch [438/500] Batch 100/133                Loss D: -0.0044,loss G: -0.8453\n",
      "Epoch [439/500] Batch 0/133                Loss D: -0.0065,loss G: -0.4784\n",
      "Epoch [439/500] Batch 100/133                Loss D: -0.0046,loss G: 0.4735\n",
      "Epoch [440/500] Batch 0/133                Loss D: -0.0109,loss G: 0.1718\n",
      "Epoch [440/500] Batch 100/133                Loss D: -0.0147,loss G: 0.5118\n",
      "Epoch [441/500] Batch 0/133                Loss D: -0.0132,loss G: -0.2400\n",
      "Epoch [441/500] Batch 100/133                Loss D: -0.0027,loss G: -0.1991\n",
      "Epoch [442/500] Batch 0/133                Loss D: -0.0025,loss G: 0.0571\n",
      "Epoch [442/500] Batch 100/133                Loss D: -0.0049,loss G: -0.0530\n",
      "Epoch [443/500] Batch 0/133                Loss D: 0.0036,loss G: -0.1672\n",
      "Epoch [443/500] Batch 100/133                Loss D: -0.0122,loss G: -0.3010\n",
      "Epoch [444/500] Batch 0/133                Loss D: -0.0112,loss G: 0.1149\n",
      "Epoch [444/500] Batch 100/133                Loss D: -0.0111,loss G: -0.6882\n",
      "Epoch [445/500] Batch 0/133                Loss D: -0.0059,loss G: -0.9980\n",
      "Epoch [445/500] Batch 100/133                Loss D: -0.0177,loss G: -1.0839\n",
      "Epoch [446/500] Batch 0/133                Loss D: -0.0179,loss G: -0.8254\n",
      "Epoch [446/500] Batch 100/133                Loss D: -0.0177,loss G: -0.2134\n",
      "Epoch [447/500] Batch 0/133                Loss D: -0.0190,loss G: -0.1051\n",
      "Epoch [447/500] Batch 100/133                Loss D: -0.0186,loss G: -0.4551\n",
      "Epoch [448/500] Batch 0/133                Loss D: -0.0223,loss G: -0.3027\n",
      "Epoch [448/500] Batch 100/133                Loss D: 0.0059,loss G: -0.2899\n",
      "Epoch [449/500] Batch 0/133                Loss D: -0.0006,loss G: -1.6496\n",
      "Epoch [449/500] Batch 100/133                Loss D: -0.0001,loss G: -0.3180\n",
      "Epoch [450/500] Batch 0/133                Loss D: -0.0040,loss G: -0.6896\n",
      "Epoch [450/500] Batch 100/133                Loss D: -0.0070,loss G: -0.8884\n",
      "Epoch [451/500] Batch 0/133                Loss D: -0.0101,loss G: -0.9255\n",
      "Epoch [451/500] Batch 100/133                Loss D: -0.0155,loss G: -0.1886\n",
      "Epoch [452/500] Batch 0/133                Loss D: -0.0190,loss G: -0.0689\n",
      "Epoch [452/500] Batch 100/133                Loss D: -0.0233,loss G: 0.2849\n",
      "Epoch [453/500] Batch 0/133                Loss D: -0.0242,loss G: 0.5089\n",
      "Epoch [453/500] Batch 100/133                Loss D: 0.0031,loss G: -0.4481\n",
      "Epoch [454/500] Batch 0/133                Loss D: 0.0012,loss G: -0.2779\n",
      "Epoch [454/500] Batch 100/133                Loss D: -0.0065,loss G: -0.7134\n",
      "Epoch [455/500] Batch 0/133                Loss D: -0.0082,loss G: -0.9926\n",
      "Epoch [455/500] Batch 100/133                Loss D: -0.0147,loss G: -0.5223\n",
      "Epoch [456/500] Batch 0/133                Loss D: -0.0102,loss G: 0.0915\n",
      "Epoch [456/500] Batch 100/133                Loss D: -0.0080,loss G: -0.3664\n",
      "Epoch [457/500] Batch 0/133                Loss D: -0.0172,loss G: -0.0421\n",
      "Epoch [457/500] Batch 100/133                Loss D: -0.0189,loss G: 0.2402\n",
      "Epoch [458/500] Batch 0/133                Loss D: -0.0171,loss G: 0.2617\n",
      "Epoch [458/500] Batch 100/133                Loss D: -0.0237,loss G: 0.9822\n",
      "Epoch [459/500] Batch 0/133                Loss D: -0.0182,loss G: 0.6523\n",
      "Epoch [459/500] Batch 100/133                Loss D: -0.0251,loss G: 0.4247\n",
      "Epoch [460/500] Batch 0/133                Loss D: -0.0255,loss G: 0.2459\n",
      "Epoch [460/500] Batch 100/133                Loss D: 0.0355,loss G: -0.6081\n",
      "Epoch [461/500] Batch 0/133                Loss D: -0.0030,loss G: -1.6020\n",
      "Epoch [461/500] Batch 100/133                Loss D: -0.0020,loss G: -1.1751\n",
      "Epoch [462/500] Batch 0/133                Loss D: -0.0069,loss G: -1.2862\n",
      "Epoch [462/500] Batch 100/133                Loss D: -0.0098,loss G: -1.9304\n",
      "Epoch [463/500] Batch 0/133                Loss D: -0.0086,loss G: -1.5849\n",
      "Epoch [463/500] Batch 100/133                Loss D: -0.0122,loss G: -1.4265\n",
      "Epoch [464/500] Batch 0/133                Loss D: -0.0160,loss G: -1.3512\n",
      "Epoch [464/500] Batch 100/133                Loss D: -0.0031,loss G: -1.4687\n",
      "Epoch [465/500] Batch 0/133                Loss D: -0.0912,loss G: -2.5529\n",
      "Epoch [465/500] Batch 100/133                Loss D: -0.0047,loss G: -1.4324\n",
      "Epoch [466/500] Batch 0/133                Loss D: -0.0034,loss G: -1.3702\n",
      "Epoch [466/500] Batch 100/133                Loss D: -0.0119,loss G: -1.9409\n",
      "Epoch [467/500] Batch 0/133                Loss D: -0.0045,loss G: -1.3745\n",
      "Epoch [467/500] Batch 100/133                Loss D: -0.0183,loss G: -1.6290\n",
      "Epoch [468/500] Batch 0/133                Loss D: -0.0148,loss G: -1.5032\n",
      "Epoch [468/500] Batch 100/133                Loss D: -0.0174,loss G: -0.4752\n",
      "Epoch [469/500] Batch 0/133                Loss D: -0.0225,loss G: -0.5137\n",
      "Epoch [469/500] Batch 100/133                Loss D: -0.0198,loss G: -0.4091\n",
      "Epoch [470/500] Batch 0/133                Loss D: -0.0222,loss G: -0.0613\n",
      "Epoch [470/500] Batch 100/133                Loss D: 0.0044,loss G: -1.5034\n",
      "Epoch [471/500] Batch 0/133                Loss D: -0.0011,loss G: -0.8103\n",
      "Epoch [471/500] Batch 100/133                Loss D: -0.0057,loss G: -1.1338\n",
      "Epoch [472/500] Batch 0/133                Loss D: -0.0064,loss G: -0.5659\n",
      "Epoch [472/500] Batch 100/133                Loss D: -0.0112,loss G: -0.5117\n",
      "Epoch [473/500] Batch 0/133                Loss D: -0.0131,loss G: -0.2526\n",
      "Epoch [473/500] Batch 100/133                Loss D: -0.0166,loss G: -0.1508\n",
      "Epoch [474/500] Batch 0/133                Loss D: -0.0160,loss G: 0.4894\n",
      "Epoch [474/500] Batch 100/133                Loss D: -0.0153,loss G: 0.4601\n",
      "Epoch [475/500] Batch 0/133                Loss D: -0.0131,loss G: 0.4633\n",
      "Epoch [475/500] Batch 100/133                Loss D: -0.0194,loss G: 1.0548\n",
      "Epoch [476/500] Batch 0/133                Loss D: -0.0206,loss G: 0.9877\n",
      "Epoch [476/500] Batch 100/133                Loss D: -0.0065,loss G: 0.5715\n",
      "Epoch [477/500] Batch 0/133                Loss D: 0.0006,loss G: -0.1532\n",
      "Epoch [477/500] Batch 100/133                Loss D: -0.0057,loss G: -0.6175\n",
      "Epoch [478/500] Batch 0/133                Loss D: -0.0031,loss G: -0.7139\n",
      "Epoch [478/500] Batch 100/133                Loss D: -0.0062,loss G: -0.5028\n",
      "Epoch [479/500] Batch 0/133                Loss D: -0.0141,loss G: -0.4413\n",
      "Epoch [479/500] Batch 100/133                Loss D: -0.0128,loss G: -0.3406\n",
      "Epoch [480/500] Batch 0/133                Loss D: -0.0168,loss G: -0.4053\n",
      "Epoch [480/500] Batch 100/133                Loss D: -0.0098,loss G: -0.7196\n",
      "Epoch [481/500] Batch 0/133                Loss D: -0.0158,loss G: -0.3728\n",
      "Epoch [481/500] Batch 100/133                Loss D: -0.0014,loss G: 1.9049\n",
      "Epoch [482/500] Batch 0/133                Loss D: -0.0066,loss G: 1.9670\n",
      "Epoch [482/500] Batch 100/133                Loss D: -0.0070,loss G: 1.4348\n",
      "Epoch [483/500] Batch 0/133                Loss D: -0.0118,loss G: 1.4192\n",
      "Epoch [483/500] Batch 100/133                Loss D: -0.0199,loss G: 1.2840\n",
      "Epoch [484/500] Batch 0/133                Loss D: -0.0180,loss G: 1.2137\n",
      "Epoch [484/500] Batch 100/133                Loss D: -0.0228,loss G: 1.3040\n",
      "Epoch [485/500] Batch 0/133                Loss D: -0.0247,loss G: 1.3813\n",
      "Epoch [485/500] Batch 100/133                Loss D: 0.0083,loss G: -0.1157\n",
      "Epoch [486/500] Batch 0/133                Loss D: 0.0012,loss G: -1.0103\n",
      "Epoch [486/500] Batch 100/133                Loss D: -0.0061,loss G: -1.5067\n",
      "Epoch [487/500] Batch 0/133                Loss D: -0.0044,loss G: -1.4794\n",
      "Epoch [487/500] Batch 100/133                Loss D: -0.0044,loss G: -1.9645\n",
      "Epoch [488/500] Batch 0/133                Loss D: -0.0085,loss G: -1.6020\n",
      "Epoch [488/500] Batch 100/133                Loss D: -0.0079,loss G: -1.5016\n",
      "Epoch [489/500] Batch 0/133                Loss D: 0.0036,loss G: -2.3771\n",
      "Epoch [489/500] Batch 100/133                Loss D: -0.0121,loss G: -1.5045\n",
      "Epoch [490/500] Batch 0/133                Loss D: -0.0130,loss G: -1.5520\n",
      "Epoch [490/500] Batch 100/133                Loss D: -0.0107,loss G: -1.1631\n",
      "Epoch [491/500] Batch 0/133                Loss D: -0.0080,loss G: -1.0059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [491/500] Batch 100/133                Loss D: -0.0119,loss G: -0.4647\n",
      "Epoch [492/500] Batch 0/133                Loss D: -0.0098,loss G: -0.3836\n",
      "Epoch [492/500] Batch 100/133                Loss D: -0.0056,loss G: -0.2806\n",
      "Epoch [493/500] Batch 0/133                Loss D: -0.0130,loss G: -0.1735\n",
      "Epoch [493/500] Batch 100/133                Loss D: -0.0162,loss G: 0.1999\n",
      "Epoch [494/500] Batch 0/133                Loss D: -0.0206,loss G: 0.1130\n",
      "Epoch [494/500] Batch 100/133                Loss D: -0.0033,loss G: -3.3836\n",
      "Epoch [495/500] Batch 0/133                Loss D: -0.0040,loss G: -4.9023\n",
      "Epoch [495/500] Batch 100/133                Loss D: -0.0180,loss G: -4.6916\n",
      "Epoch [496/500] Batch 0/133                Loss D: -0.0126,loss G: -3.8746\n",
      "Epoch [496/500] Batch 100/133                Loss D: -0.0039,loss G: -3.2023\n",
      "Epoch [497/500] Batch 0/133                Loss D: -0.0053,loss G: -2.7725\n",
      "Epoch [497/500] Batch 100/133                Loss D: -0.0206,loss G: -3.4384\n",
      "Epoch [498/500] Batch 0/133                Loss D: -0.0073,loss G: -2.8940\n",
      "Epoch [498/500] Batch 100/133                Loss D: -0.0202,loss G: -1.6664\n",
      "Epoch [499/500] Batch 0/133                Loss D: -0.0190,loss G: -2.0693\n",
      "Epoch [499/500] Batch 100/133                Loss D: -0.0163,loss G: -1.3514\n"
     ]
    }
   ],
   "source": [
    "gen.train()\n",
    "critic.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "     \n",
    "    for batch_idx,data in enumerate(loader):\n",
    "        real = data[1]\n",
    "        real = real.to(device)\n",
    "        cur_batch_size = real.shape[0]\n",
    "        labels = data[0]\n",
    "        labels = labels.to(device)\n",
    "#     for batch_idx, (real,labels) in enumerate(loader):\n",
    "#         real = real.to(device)\n",
    "#         cur_batch_size = real.shape[0]\n",
    "#         labels = labels.to(device)\n",
    "        \n",
    "        for _ in range(CRITIC_ITERATIONS):\n",
    "            noise = torch.randn((cur_batch_size,Z_DIM,1,1)).to(device)\n",
    "            fake = gen(noise,labels)\n",
    "            critic_real = critic(real,labels).reshape(-1)\n",
    "            critic_fake = critic(fake,labels).reshape(-1)\n",
    "            gp = gradient_penalty(critic,labels,real,fake,device=device)\n",
    "            loss_critic = ( -(torch.mean(critic_real)-torch.mean(critic_fake)) +LAMBDA_GP*gp)\n",
    "            critic.zero_grad()\n",
    "            loss_critic.backward(retain_graph = True)\n",
    "            opt_critic.step()\n",
    "     \n",
    "        \n",
    "        ###训练生成器 min log(1-D(G(z))) <----->max log(D(G(z))) \n",
    "        gen_fake = critic(fake,labels).reshape(-1)\n",
    "        loss_gen = -torch.mean(gen_fake)        \n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "        \n",
    "        \n",
    "        # Print losses osccasionally and print to tensorboard\n",
    "        if batch_idx %100==0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(loader)}\\\n",
    "                Loss D: {loss_critic:.4f},loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                fake = gen(noise,labels)\n",
    "                #take out(up to) 32 examples\n",
    "                img_grid_real = torchvision.utils.make_grid(\n",
    "                    real[:32],normalize = True\n",
    "                \n",
    "                )\n",
    "                \n",
    "                img_grid_fake = torchvision.utils.make_grid(\n",
    "                    fake[:32],normalize = True\n",
    "                \n",
    "                )\n",
    "                \n",
    "                writer_real.add_image(\"Real\",img_grid_real,global_step=step)\n",
    "                writer_fake.add_image(\"Fake\",img_grid_fake,global_step=step)\n",
    "                \n",
    "            step +=1\n",
    "                   \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试生成器的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 67, 67])\n"
     ]
    }
   ],
   "source": [
    "out_noise = torch.randn((100,100,1,1))\n",
    "out_1 = torch.ones(100)\n",
    "out_1 = torch.FloatTensor(out_1).long()\n",
    "out_noise=out_noise.to(device)\n",
    "out_1 = out_1.to(device)\n",
    "fake1 = gen(out_noise,out_1).to(device)\n",
    "print(fake1.shape)\n",
    "L_1 = fake1.view(100,67*67)*6108\n",
    "L_1=L_1.cpu().detach().numpy().tolist()\n",
    "np.savetxt('./L1.csv',L_1,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 67, 67])\n"
     ]
    }
   ],
   "source": [
    "out_noise = torch.randn((100,100,1,1))\n",
    "out_0 = torch.zeros(100)\n",
    "out_0 = torch.FloatTensor(out_0).long()\n",
    "out_noise=out_noise.to(device)\n",
    "out_0 = out_0.to(device)\n",
    "fake0 = gen(out_noise,out_0).to(device)\n",
    "print(fake0.shape)\n",
    "L_0 = fake0.view(100,67*67)*6108\n",
    "L_0=L_0.cpu().detach().numpy().tolist()\n",
    "np.savetxt('./L0.csv',L_0,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 67, 67])\n"
     ]
    }
   ],
   "source": [
    "batch_out_data = 1000\n",
    "out_noise = torch.randn((batch_out_data,100,1,1))\n",
    "out_1 = torch.ones(batch_out_data)\n",
    "out_1 = torch.FloatTensor(out_1).long()\n",
    "out_noise=out_noise.to(device)\n",
    "out_1 = out_1.to(device)\n",
    "fake1 = gen(out_noise,out_1).to(device)\n",
    "print(fake1.shape)\n",
    "L_1 = fake1.view(batch_out_data,67*67)*6108\n",
    "L_1=L_1.cpu().detach().numpy().tolist()\n",
    "np.savetxt('./L1000_1.csv',L_1,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 67, 67])\n"
     ]
    }
   ],
   "source": [
    "batch_out_data = 1000\n",
    "out_noise = torch.randn((batch_out_data,100,1,1))\n",
    "out_0 = torch.zeros(batch_out_data)\n",
    "out_0 = torch.FloatTensor(out_0).long()\n",
    "out_noise=out_noise.to(device)\n",
    "out_0 = out_0.to(device)\n",
    "fake0 = gen(out_noise,out_0).to(device)\n",
    "print(fake0.shape)\n",
    "L_0 = fake0.view(batch_out_data,67*67)*6108\n",
    "L_0=L_0.cpu().detach().numpy().tolist()\n",
    "np.savetxt('./L1000_0.csv',L_0,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
